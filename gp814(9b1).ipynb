{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from ark_nlp.model.ner.global_pointer_bert import Tokenizer\n",
    "from ark_nlp.model.ner.global_pointer_bert import Dataset as Dt\n",
    "import os\n",
    "import jieba\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import logging\n",
    "from nezha import NeZhaConfig, NeZhaModel, NeZhaForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机种子： 1998\n",
      "batch_size:  8\n",
      "学习率： 4e-05\n",
      "./my_nezha_cn_base2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./my_nezha_cn_base2/ were not used when initializing NeZhaModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing NeZhaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NeZhaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of NeZhaModel were not initialized from the model checkpoint at ./my_nezha_cn_base2/ and are newly initialized: ['bert.encoder.layer.8.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.4.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.3.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.1.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.2.attention.self.relative_positions_encoding.positions_encoding', 'bert.pooler.dense.bias', 'bert.encoder.layer.9.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.0.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.6.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.5.attention.self.relative_positions_encoding.positions_encoding', 'bert.pooler.dense.weight', 'bert.encoder.layer.11.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.10.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.7.attention.self.relative_positions_encoding.positions_encoding']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # GPU\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # 禁止hash随机化\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # True的话会自动寻找最适合当前配置的高效算法，来达到优化运行效率的问题。False保证实验结果可复现\n",
    "\n",
    "# 设置随机数种子\n",
    "RANDOM_SEED = 1998\n",
    "print('随机种子：', RANDOM_SEED)\n",
    "setup_seed(RANDOM_SEED)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.set_verbosity_warning()\n",
    "\n",
    "dropout_num = 0\n",
    "start = time.time()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAX_LEN = 128\n",
    "batch_size = 8\n",
    "print('batch_size: ', batch_size)\n",
    "EPOCHS = 6\n",
    "LR = 4e-5\n",
    "print('学习率：', LR)\n",
    "early_stop_epochs = 2\n",
    "model_path = './my_nezha_cn_base2/'\n",
    "model_save_path = 'model'\n",
    "print(model_path)\n",
    "\n",
    "\n",
    "\n",
    "config = NeZhaConfig.from_json_file(model_path + 'config.json')\n",
    "config.num_labels = 53\n",
    "config.output_hidden_states = True\n",
    "# encoder = NeZhaForSequenceClassification(config)\n",
    "encoder = NeZhaModel.from_pretrained(model_path, config=config)\n",
    "\n",
    "import os\n",
    "from ark_nlp.factory.utils.conlleval import get_entity_bio"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n"
     ]
    }
   ],
   "source": [
    "datalist = []\n",
    "with open('./datasets/preliminary_contest_datasets/train_data/train.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    lines.append('\\n')\n",
    "\n",
    "    text = []\n",
    "    labels = []\n",
    "    label_set = set()\n",
    "\n",
    "    for line in lines:\n",
    "        if line == '\\n':\n",
    "            text = ''.join(text)\n",
    "            entity_labels = []\n",
    "            for _type, _start_idx, _end_idx in get_entity_bio(labels, id2label=None):\n",
    "                entity_labels.append({\n",
    "                    'start_idx': _start_idx,\n",
    "                    'end_idx': _end_idx,\n",
    "                    'type': _type,\n",
    "                    'entity': text[_start_idx: _end_idx + 1]\n",
    "                })\n",
    "\n",
    "            if text == '':\n",
    "                continue\n",
    "\n",
    "            datalist.append({\n",
    "                'text': text,\n",
    "                'label': entity_labels,\n",
    "                'BIO': labels\n",
    "            })\n",
    "\n",
    "            text = []\n",
    "            labels = []\n",
    "\n",
    "        elif line == '  O\\n':\n",
    "            text.append(' ')\n",
    "            labels.append('O')\n",
    "        else:\n",
    "            line = line.strip('\\n').split()\n",
    "            if len(line) == 1:\n",
    "                term = ' '\n",
    "                label = line[0]\n",
    "            else:\n",
    "                term, label = line\n",
    "            text.append(term)\n",
    "            label_set.add(label.split('-')[-1])\n",
    "            labels.append(label)\n",
    "\n",
    "\n",
    "\n",
    "# aug_datalist = []\n",
    "# with open('./aug_train_data.txt', 'r', encoding='utf-8') as f:\n",
    "#     lines = f.readlines()\n",
    "#     lines.append('\\n')\n",
    "#\n",
    "#     text = []\n",
    "#     labels = []\n",
    "#\n",
    "#     for line in lines:\n",
    "#         if line == '\\n':\n",
    "#             text = ''.join(text)\n",
    "#             entity_labels = []\n",
    "#             for _type, _start_idx, _end_idx in get_entity_bio(labels, id2label=None):\n",
    "#                 entity_labels.append({\n",
    "#                     'start_idx': _start_idx,\n",
    "#                     'end_idx': _end_idx,\n",
    "#                     'type': _type,\n",
    "#                     'entity': text[_start_idx: _end_idx + 1]\n",
    "#                 })\n",
    "#\n",
    "#             if text == '':\n",
    "#                 continue\n",
    "#\n",
    "#             aug_datalist .append({\n",
    "#                 'text': text,\n",
    "#                 'label': entity_labels,\n",
    "#                 'BIO': labels\n",
    "#             })\n",
    "#\n",
    "#             text = []\n",
    "#             labels = []\n",
    "#\n",
    "#         elif line == '  O\\n':\n",
    "#             text.append(' ')\n",
    "#             labels.append('O')\n",
    "#         else:\n",
    "#             line = line.strip('\\n').split()\n",
    "#             if len(line) == 1:\n",
    "#                 term = ' '\n",
    "#                 label = line[0]\n",
    "#             else:\n",
    "#                 term, label = line\n",
    "#             text.append(term)\n",
    "#             labels.append(label)\n",
    "\n",
    "# aug_all_data = pd.DataFrame(aug_datalist)\n",
    "# print('增强数据大小：', len(aug_all_data))\n",
    "\n",
    "\n",
    "print(len(datalist))\n",
    "# 这里随意分割了一下看指标，建议实际使用sklearn分割或者交叉验证\n",
    "#datalist =  datalist[:10000]\n",
    "#train_data_df = pd.DataFrame(datalist)\n",
    "# print('训练集大小：', len(train_data_df))\n",
    "# train_data_df['label'] = train_data_df['label'].apply(lambda x: str(x))\n",
    "#\n",
    "#dev_data_df = pd.DataFrame(datalist[-400:])\n",
    "# dev_data_df['label'] = dev_data_df['label'].apply(lambda x: str(x))\n",
    "\n",
    "all_data = pd.DataFrame(datalist)\n",
    "\n",
    "#pseudo_data = all_data[:11122]\n",
    "#all_data = all_data[11122:]\n",
    "# all_data = all_data[:100]\n",
    "\n",
    "\n",
    "train_data_df, dev_data_df = train_test_split(all_data, test_size = 0.1, random_state=RANDOM_SEED)\n",
    "# print('训练集大小：', len(train_data_df))\n",
    "# print('验证集大小：', len(dev_data_df))\n",
    "#train_data_df = pd.concat([train_data_df,pseudo_data])\n",
    "# aug_train_data_df, aug_dev_data_df = train_test_split(aug_all_data, test_size = 0.1, random_state=RANDOM_SEED)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    text  \\\n37808               抽屉式桌面收纳盒整理盒透明塑料化妆品收纳办公桌杂物文具收纳箱 A5单层低   \n10193  A4卡纸 250g加厚硬卡纸儿童幼儿园彩纸手工纸a3黑白彩色卡纸 230g A4硬卡纸咖啡色...   \n24264  同款小学生流沙笔袋男多功能铅笔盒大容量简约小清新男孩笔盒笔袋中学铅笔盒同款男生文具盒 加长敖...   \n25931  白色卡纸a3纸绘画纸加厚卡纸A4卡纸手工120克160克230克卡纸8k硬卡纸4k美术绘画纸...   \n28975  戴尔（DELL） Optiplex3060SFF 迷你办公电脑商用台式机i3微型小主机 30...   \n\n                                                   label  \\\n37808  [{'start_idx': 0, 'end_idx': 2, 'type': '13', ...   \n10193  [{'start_idx': 0, 'end_idx': 1, 'type': '18', ...   \n24264  [{'start_idx': 0, 'end_idx': 1, 'type': '13', ...   \n25931  [{'start_idx': 0, 'end_idx': 1, 'type': '16', ...   \n28975  [{'start_idx': 0, 'end_idx': 1, 'type': '1', '...   \n\n                                                     BIO  \n37808  [B-13, I-13, I-13, B-4, I-4, I-4, I-4, I-4, B-...  \n10193  [B-18, I-18, B-4, I-4, O, B-18, I-18, I-18, I-...  \n24264  [B-13, I-13, B-8, I-8, I-8, B-13, I-13, B-4, I...  \n25931  [B-16, I-16, B-4, I-4, B-4, I-4, I-4, B-4, I-4...  \n28975  [B-1, I-1, O, B-1, I-1, I-1, I-1, O, O, O, O, ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>BIO</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>37808</th>\n      <td>抽屉式桌面收纳盒整理盒透明塑料化妆品收纳办公桌杂物文具收纳箱 A5单层低</td>\n      <td>[{'start_idx': 0, 'end_idx': 2, 'type': '13', ...</td>\n      <td>[B-13, I-13, I-13, B-4, I-4, I-4, I-4, I-4, B-...</td>\n    </tr>\n    <tr>\n      <th>10193</th>\n      <td>A4卡纸 250g加厚硬卡纸儿童幼儿园彩纸手工纸a3黑白彩色卡纸 230g A4硬卡纸咖啡色...</td>\n      <td>[{'start_idx': 0, 'end_idx': 1, 'type': '18', ...</td>\n      <td>[B-18, I-18, B-4, I-4, O, B-18, I-18, I-18, I-...</td>\n    </tr>\n    <tr>\n      <th>24264</th>\n      <td>同款小学生流沙笔袋男多功能铅笔盒大容量简约小清新男孩笔盒笔袋中学铅笔盒同款男生文具盒 加长敖...</td>\n      <td>[{'start_idx': 0, 'end_idx': 1, 'type': '13', ...</td>\n      <td>[B-13, I-13, B-8, I-8, I-8, B-13, I-13, B-4, I...</td>\n    </tr>\n    <tr>\n      <th>25931</th>\n      <td>白色卡纸a3纸绘画纸加厚卡纸A4卡纸手工120克160克230克卡纸8k硬卡纸4k美术绘画纸...</td>\n      <td>[{'start_idx': 0, 'end_idx': 1, 'type': '16', ...</td>\n      <td>[B-16, I-16, B-4, I-4, B-4, I-4, I-4, B-4, I-4...</td>\n    </tr>\n    <tr>\n      <th>28975</th>\n      <td>戴尔（DELL） Optiplex3060SFF 迷你办公电脑商用台式机i3微型小主机 30...</td>\n      <td>[{'start_idx': 0, 'end_idx': 1, 'type': '1', '...</td>\n      <td>[B-1, I-1, O, B-1, I-1, I-1, I-1, O, O, O, O, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data_df.head(5)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小： 36000\n",
      "验证集大小： 4000\n"
     ]
    }
   ],
   "source": [
    "train_data_df.index = list(range(len(train_data_df)))\n",
    "\n",
    "dev_data_df.index = list(range(len(dev_data_df)))\n",
    "\n",
    "\n",
    "# 伪标数量\n",
    "# train_data_df = pd.concat([train_data_df,aug_train_data_df])\n",
    "# train_data_df.index = list(range(len(train_data_df)))\n",
    "\n",
    "train_data_df['label'] = train_data_df['label'].apply(lambda x: str(x))\n",
    "dev_data_df['label'] = dev_data_df['label'].apply(lambda x: str(x))\n",
    "\n",
    "print('训练集大小：', len(train_data_df))\n",
    "print('验证集大小：', len(dev_data_df))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  \\\n0               抽屉式桌面收纳盒整理盒透明塑料化妆品收纳办公桌杂物文具收纳箱 A5单层低   \n1  A4卡纸 250g加厚硬卡纸儿童幼儿园彩纸手工纸a3黑白彩色卡纸 230g A4硬卡纸咖啡色...   \n2  同款小学生流沙笔袋男多功能铅笔盒大容量简约小清新男孩笔盒笔袋中学铅笔盒同款男生文具盒 加长敖...   \n3  白色卡纸a3纸绘画纸加厚卡纸A4卡纸手工120克160克230克卡纸8k硬卡纸4k美术绘画纸...   \n4  戴尔（DELL） Optiplex3060SFF 迷你办公电脑商用台式机i3微型小主机 30...   \n\n                                               label  \\\n0  [{'start_idx': 0, 'end_idx': 2, 'type': '13', ...   \n1  [{'start_idx': 0, 'end_idx': 1, 'type': '18', ...   \n2  [{'start_idx': 0, 'end_idx': 1, 'type': '13', ...   \n3  [{'start_idx': 0, 'end_idx': 1, 'type': '16', ...   \n4  [{'start_idx': 0, 'end_idx': 1, 'type': '1', '...   \n\n                                                 BIO  \n0  [B-13, I-13, I-13, B-4, I-4, I-4, I-4, I-4, B-...  \n1  [B-18, I-18, B-4, I-4, O, B-18, I-18, I-18, I-...  \n2  [B-13, I-13, B-8, I-8, I-8, B-13, I-13, B-4, I...  \n3  [B-16, I-16, B-4, I-4, B-4, I-4, I-4, B-4, I-4...  \n4  [B-1, I-1, O, B-1, I-1, I-1, I-1, O, O, O, O, ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>BIO</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>抽屉式桌面收纳盒整理盒透明塑料化妆品收纳办公桌杂物文具收纳箱 A5单层低</td>\n      <td>[{'start_idx': 0, 'end_idx': 2, 'type': '13', ...</td>\n      <td>[B-13, I-13, I-13, B-4, I-4, I-4, I-4, I-4, B-...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A4卡纸 250g加厚硬卡纸儿童幼儿园彩纸手工纸a3黑白彩色卡纸 230g A4硬卡纸咖啡色...</td>\n      <td>[{'start_idx': 0, 'end_idx': 1, 'type': '18', ...</td>\n      <td>[B-18, I-18, B-4, I-4, O, B-18, I-18, I-18, I-...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>同款小学生流沙笔袋男多功能铅笔盒大容量简约小清新男孩笔盒笔袋中学铅笔盒同款男生文具盒 加长敖...</td>\n      <td>[{'start_idx': 0, 'end_idx': 1, 'type': '13', ...</td>\n      <td>[B-13, I-13, B-8, I-8, I-8, B-13, I-13, B-4, I...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>白色卡纸a3纸绘画纸加厚卡纸A4卡纸手工120克160克230克卡纸8k硬卡纸4k美术绘画纸...</td>\n      <td>[{'start_idx': 0, 'end_idx': 1, 'type': '16', ...</td>\n      <td>[B-16, I-16, B-4, I-4, B-4, I-4, I-4, B-4, I-4...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>戴尔（DELL） Optiplex3060SFF 迷你办公电脑商用台式机i3微型小主机 30...</td>\n      <td>[{'start_idx': 0, 'end_idx': 1, 'type': '1', '...</td>\n      <td>[B-1, I-1, O, B-1, I-1, I-1, I-1, O, O, O, O, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data_df.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'start_idx': 0, 'end_idx': 2, 'type': '13', 'entity': '抽屉式'},\n {'start_idx': 3, 'end_idx': 7, 'type': '4', 'entity': '桌面收纳盒'},\n {'start_idx': 8, 'end_idx': 10, 'type': '4', 'entity': '整理盒'},\n {'start_idx': 11, 'end_idx': 12, 'type': '16', 'entity': '透明'},\n {'start_idx': 13, 'end_idx': 14, 'type': '12', 'entity': '塑料'},\n {'start_idx': 15, 'end_idx': 17, 'type': '40', 'entity': '化妆品'},\n {'start_idx': 18, 'end_idx': 19, 'type': '5', 'entity': '收纳'},\n {'start_idx': 20, 'end_idx': 22, 'type': '7', 'entity': '办公桌'},\n {'start_idx': 23, 'end_idx': 24, 'type': '9', 'entity': '杂物'},\n {'start_idx': 25, 'end_idx': 26, 'type': '40', 'entity': '文具'},\n {'start_idx': 27, 'end_idx': 29, 'type': '4', 'entity': '收纳箱'},\n {'start_idx': 31, 'end_idx': 32, 'type': '18', 'entity': 'A5'},\n {'start_idx': 33, 'end_idx': 34, 'type': '18', 'entity': '单层'},\n {'start_idx': 35, 'end_idx': 35, 'type': '13', 'entity': '低'}]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data_df['label'].iloc[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "{'1',\n '10',\n '11',\n '12',\n '13',\n '14',\n '15',\n '16',\n '17',\n '18',\n '19',\n '2',\n '20',\n '21',\n '22',\n '23',\n '24',\n '25',\n '26',\n '28',\n '29',\n '3',\n '30',\n '31',\n '32',\n '33',\n '34',\n '35',\n '36',\n '37',\n '38',\n '39',\n '4',\n '40',\n '41',\n '42',\n '43',\n '44',\n '46',\n '47',\n '48',\n '49',\n '5',\n '50',\n '51',\n '52',\n '53',\n '54',\n '6',\n '7',\n '8',\n '9',\n 'O'}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "\"[{'start_idx': 0, 'end_idx': 2, 'type': '13', 'entity': '抽屉式'}, {'start_idx': 3, 'end_idx': 7, 'type': '4', 'entity': '桌面收纳盒'}, {'start_idx': 8, 'end_idx': 10, 'type': '4', 'entity': '整理盒'}, {'start_idx': 11, 'end_idx': 12, 'type': '16', 'entity': '透明'}, {'start_idx': 13, 'end_idx': 14, 'type': '12', 'entity': '塑料'}, {'start_idx': 15, 'end_idx': 17, 'type': '40', 'entity': '化妆品'}, {'start_idx': 18, 'end_idx': 19, 'type': '5', 'entity': '收纳'}, {'start_idx': 20, 'end_idx': 22, 'type': '7', 'entity': '办公桌'}, {'start_idx': 23, 'end_idx': 24, 'type': '9', 'entity': '杂物'}, {'start_idx': 25, 'end_idx': 26, 'type': '40', 'entity': '文具'}, {'start_idx': 27, 'end_idx': 29, 'type': '4', 'entity': '收纳箱'}, {'start_idx': 31, 'end_idx': 32, 'type': '18', 'entity': 'A5'}, {'start_idx': 33, 'end_idx': 34, 'type': '18', 'entity': '单层'}, {'start_idx': 35, 'end_idx': 35, 'type': '13', 'entity': '低'}]\""
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data_df['label'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "label_list = sorted(list(label_set))\n",
    "label_list = ['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36',\n",
    "            '37', '38', '39', '4', '40', '41', '42', '43', '44', '46', '47', '48', '49', '5', '50', '51', '52', '53', '54', '6', '7', '8', '9', 'O']\n",
    "\n",
    "\n",
    "del text\n",
    "del label_set\n",
    "del labels\n",
    "train_dataset = Dt(train_data_df, categories=label_list)\n",
    "dev_dataset = Dt(dev_data_df, categories=label_list)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "ark_tokenizer = Tokenizer(vocab=tokenizer, max_seq_len=128)\n",
    "\n",
    "train_dataset.convert_to_ids(ark_tokenizer)\n",
    "dev_dataset.convert_to_ids(ark_tokenizer)\n",
    "\n",
    "\n",
    "Ent2id = train_dataset.cat2id  # 53\n",
    "id2Ent = train_dataset.id2cat\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ent2id len:  53\n"
     ]
    }
   ],
   "source": [
    "train_dataset.cat2id = {'1': 0,'10': 1, '11': 2, '12': 3, '13': 4, '14': 5, '15': 6, '16': 7, '17': 8, '18': 9,\n",
    " '19': 10, '2': 11, '20': 12, '21': 13, '22': 14, '23': 15, '24': 16, '25': 17, '26': 18, '28': 19,\n",
    "'29': 20, '3': 21, '30': 22, '31': 23, '32': 24, '33': 25, '34': 26, '35': 27, '36': 28, '37': 29,\n",
    "'38': 30, '39': 31, '4': 32, '40': 33, '41': 34, '42': 35, '43': 36, '44': 37, '46': 38, '47': 39,\n",
    "'48': 40, '49': 41, '5': 42, '50': 43, '51': 44, '52': 45, '53': 46, '54': 47, '6': 48, '7': 49, '8': 50, '9': 51, 'O': 52}\n",
    "Ent2id = train_dataset.cat2id\n",
    "\n",
    "\n",
    "print('Ent2id len: ', len(Ent2id))\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len, ark_data):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.ark_data = ark_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        token_ids, at_mask = self.get_token_ids(row)\n",
    "        return torch.tensor(token_ids, dtype=torch.long), torch.tensor(at_mask, dtype=torch.long), torch.tensor(\n",
    "            self.ark_data[index]['label_ids'].to_dense()), torch.tensor(self.ark_data[index]['token_type_ids'],\n",
    "                                                                        dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_token_ids(self, row):\n",
    "        sentence = row.text\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "        padding = [0] * (self.max_len - len(tokens))\n",
    "        at_mask = [1] * len(tokens)\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        token_ids = token_ids + padding\n",
    "        at_mask = at_mask + padding\n",
    "        return token_ids, at_mask\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        token_ids = torch.stack([x[0] for x in batch])\n",
    "        at_mask = torch.stack([x[1] for x in batch])\n",
    "        labels = torch.stack([x[2] for x in batch])\n",
    "        token_type_ids = torch.stack([x[3] for x in batch])\n",
    "        return token_ids, at_mask, labels.squeeze(), token_type_ids\n",
    "\n",
    "\n",
    "ner_train_dataset = Dataset(train_data_df, ark_tokenizer, MAX_LEN, train_dataset)\n",
    "ner_dev_dataset = Dataset(dev_data_df, ark_tokenizer, MAX_LEN, dev_dataset)\n",
    "\n",
    "train_loader = DataLoader(ner_train_dataset,  # 1250\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=ner_train_dataset.collate_fn)\n",
    "dev_loader = DataLoader(ner_dev_dataset,  # 13\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        collate_fn=ner_dev_dataset.collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        ...,\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class FGM(object):\n",
    "    def __init__(self, module):\n",
    "        self.module = module\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(\n",
    "            self,\n",
    "            epsilon=1.,\n",
    "            emb_name='word_embeddings'\n",
    "    ):\n",
    "        for name, param in self.module.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0 and not torch.isnan(norm):\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(\n",
    "            self,\n",
    "            emb_name='word_embeddings'\n",
    "    ):\n",
    "        for name, param in self.module.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "\n",
    "\n",
    "class PGD():\n",
    "    def __init__(self, model, emb_name='word_embeddings', epsilon=1., alpha=0.3):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        self.model = model\n",
    "        self.emb_name = emb_name\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.emb_backup = {}\n",
    "        self.grad_backup = {}\n",
    "\n",
    "    def attack(self, is_first_attack=False):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and self.emb_name in name:\n",
    "                if is_first_attack:\n",
    "                    self.emb_backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0:\n",
    "                    r_at = self.alpha * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "                    param.data = self.project(name, param.data, self.epsilon)\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and self.emb_name in name:\n",
    "                assert name in self.emb_backup\n",
    "                param.data = self.emb_backup[name]\n",
    "        self.emb_backup = {}\n",
    "\n",
    "    def project(self, param_name, param_data, epsilon):\n",
    "        r = param_data - self.emb_backup[param_name]\n",
    "        if torch.norm(r) > epsilon:\n",
    "            r = epsilon * r / torch.norm(r)\n",
    "        return self.emb_backup[param_name] + r\n",
    "\n",
    "    def backup_grad(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                self.grad_backup[name] = param.grad.clone()\n",
    "\n",
    "    def restore_grad(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                param.grad = self.grad_backup[name]\n",
    "\n",
    "\n",
    "class EMA():\n",
    "    def __init__(self, model, decay):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "\n",
    "    def register(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def apply_shadow(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                self.backup[name] = param.data\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "\n",
    "\n",
    "class GlobalPointerCrossEntropy(nn.Module):\n",
    "    '''Multi-class Focal loss implementation'''\n",
    "\n",
    "    def __init__(self, ):\n",
    "        super(GlobalPointerCrossEntropy, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def multilabel_categorical_crossentropy(y_true, y_pred):\n",
    "        y_pred = (1 - 2 * y_true) * y_pred\n",
    "        y_pred_neg = y_pred - y_true * 1e12\n",
    "        y_pred_pos = y_pred - (1 - y_true) * 1e12\n",
    "        zeros = torch.zeros_like(y_pred[..., :1])\n",
    "        y_pred_neg = torch.cat([y_pred_neg, zeros], dim=-1)\n",
    "        y_pred_pos = torch.cat([y_pred_pos, zeros], dim=-1)\n",
    "        neg_loss = torch.logsumexp(y_pred_neg, dim=-1)\n",
    "        pos_loss = torch.logsumexp(y_pred_pos, dim=-1)\n",
    "\n",
    "        return neg_loss + pos_loss\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        logits: [N, C, L, L]\n",
    "        \"\"\"\n",
    "        bh = logits.shape[0] * logits.shape[1]\n",
    "        target = torch.reshape(target, (bh, -1))\n",
    "        logits = torch.reshape(logits, (bh, -1))\n",
    "        return torch.mean(GlobalPointerCrossEntropy.multilabel_categorical_crossentropy(target, logits))\n",
    "\n",
    "\n",
    "class MetricsCalculator(object):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_sample_f1(self, y_pred, y_true):\n",
    "        y_pred = torch.gt(y_pred, 0).float()\n",
    "        return 2 * torch.sum(y_true * y_pred) / torch.sum(y_true + y_pred)\n",
    "\n",
    "    def get_sample_precision(self, y_pred, y_true):\n",
    "        y_pred = torch.gt(y_pred, 0).float()\n",
    "        return torch.sum(y_pred[y_true == 1]) / (y_pred.sum() + 1)\n",
    "\n",
    "    def get_evaluate_fpr(self, y_pred, y_true):\n",
    "        y_pred = y_pred.cpu().numpy()\n",
    "        y_true = y_true.cpu().numpy()\n",
    "        pred = []\n",
    "        true = []\n",
    "        for b, l, start, end in zip(*np.where(y_pred > 0)):\n",
    "            pred.append((b, l, start, end))\n",
    "        for b, l, start, end in zip(*np.where(y_true > 0)):\n",
    "            true.append((b, l, start, end))\n",
    "\n",
    "        R = set(pred)\n",
    "        T = set(true)\n",
    "        X = len(R & T)\n",
    "        Y = len(R)\n",
    "        Z = len(T)\n",
    "        if Y != 0:\n",
    "            f1, precision, recall = 2 * X / (Y + Z), X / Y, X / Z\n",
    "        else:\n",
    "            f1, precision, recall = 2 * X / (Y + Z), 0, X / Z\n",
    "        return f1, precision, recall\n",
    "\n",
    "\n",
    "class GlobalPointer(nn.Module):\n",
    "    def __init__(self, encoder, ent_type_size, inner_dim, RoPE=True):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.ent_type_size = ent_type_size\n",
    "        self.inner_dim = inner_dim\n",
    "        self.hidden_size = encoder.config.hidden_size\n",
    "        self.dense = nn.Linear(self.hidden_size*4+256, self.ent_type_size * self.inner_dim * 2)\n",
    "        self.gru = nn.GRU(input_size=768,\n",
    "                          hidden_size=384,\n",
    "                          num_layers=1,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=True)\n",
    "        self.n_gram_fc = nn.Linear(768, 256)\n",
    "        self.n_gram_tanh = nn.Tanh()\n",
    "\n",
    "        #\n",
    "        # self.fc2 = nn.Linear(768, 768)\n",
    "        # self.tanh2 = nn.Tanh()\n",
    "        # self.last_gru = nn.GRU(input_size=self.hidden_size*2+256,\n",
    "        #                   hidden_size=768,\n",
    "        #                   num_layers=1,\n",
    "        #                   batch_first=True,\n",
    "        #                   bidirectional=True)\n",
    "\n",
    "        self.RoPE = RoPE\n",
    "\n",
    "    def sinusoidal_position_embedding(self, batch_size, seq_len, output_dim):\n",
    "        position_ids = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(-1)\n",
    "\n",
    "        indices = torch.arange(0, output_dim // 2, dtype=torch.float)\n",
    "        indices = torch.pow(10000, -2 * indices / output_dim)\n",
    "        embeddings = position_ids * indices\n",
    "        embeddings = torch.stack([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)\n",
    "        embeddings = embeddings.repeat((batch_size, *([1] * len(embeddings.shape))))\n",
    "        embeddings = torch.reshape(embeddings, (batch_size, seq_len, output_dim))\n",
    "        embeddings = embeddings.to(self.device)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_ngram_feats(self, x, ngram_range=1):\n",
    "        # ngram_range = 1表示取前后token 1个\n",
    "        n_gram_feats = []\n",
    "        for idx, i in enumerate(x):\n",
    "            if idx - ngram_range < 0 and idx + ngram_range > len(x) - 1:\n",
    "                temp = list()\n",
    "                for tidx in range(idx-ngram_range, idx+ngram_range+1):\n",
    "                    temp.append(x[tidx])\n",
    "                n_gram_feats.append(temp)\n",
    "            elif idx - ngram_range < 0:\n",
    "                temp = list()\n",
    "                for tidx in range(0, idx+ngram_range+1):\n",
    "                    temp.append(x[tidx])\n",
    "                n_gram_feats.append(temp)\n",
    "            elif idx + ngram_range > len(x) - 1:\n",
    "                temp = list()\n",
    "                for tidx in range(idx-ngram_range, len(x)):\n",
    "                    temp.append(x[tidx])\n",
    "                n_gram_feats.append(temp)\n",
    "            else:\n",
    "                temp = list()\n",
    "                for tidx in range(idx-ngram_range, idx+ngram_range+1):\n",
    "                    temp.append(x[tidx])\n",
    "                n_gram_feats.append(temp)\n",
    "\n",
    "        return n_gram_feats\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        self.device = input_ids.device\n",
    "\n",
    "        context_outputs = self.encoder(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "        #         print(context_outputs[0].size())  (batch_size,max_len,hidden_size)\n",
    "        #         print(context_outputs[1].size())  (batch_size,hidden_size)\n",
    "        # last_hidden_state:(batch_size, seq_len, hidden_size)\n",
    "        last_hidden_state = context_outputs[0]\n",
    "        # last_hidden_state.shape = (bs, seq_len, 768)\n",
    "        # print('fuck')\n",
    "        # print(len(context_outputs))\n",
    "        # print(len(context_outputs[2]))\n",
    "        # print(context_outputs[2][1].shape)\n",
    "        # print(context_outputs[0]==context_outputs[2][12]) True\n",
    "\n",
    "\n",
    "\n",
    "        batch_size = last_hidden_state.size()[0]\n",
    "        seq_len = last_hidden_state.size()[1]\n",
    "\n",
    "\n",
    "        emb_state = context_outputs[2][0]\n",
    "        temp_emb_hidden_state = emb_state[:,1:MAX_LEN-1,:]\n",
    "        # temp_last_hidden_state.shape = (bs, MAX_LEN-2, 768)\n",
    "        # print('temp_last_hidden_state: ', temp_last_hidden_state.shape)\n",
    "        n_gram_feats_idx = self.get_ngram_feats(list(range(temp_emb_hidden_state.shape[1])), ngram_range=2)\n",
    "        n_gram_feats = []\n",
    "\n",
    "        for n_gram in n_gram_feats_idx:\n",
    "\n",
    "            temp = temp_emb_hidden_state[:, n_gram[0], :]\n",
    "            for i in range(1, len(n_gram)):\n",
    "                # temp += temp_last_hidden_state[:, n_gram[i], :]\n",
    "                temp = torch.add(temp, temp_emb_hidden_state[:, n_gram[i], :])\n",
    "            n_gram_feats.append(temp)\n",
    "\n",
    "\n",
    "        n_gram_feats = torch.stack(n_gram_feats, dim=1)\n",
    "\n",
    "\n",
    "        n_gram_feats = self.n_gram_tanh(self.n_gram_fc(n_gram_feats))\n",
    "\n",
    "        # (bs, MAX_LEN-2, 256)\n",
    "\n",
    "\n",
    "        # last_hidden_state = torch.cat((last_hidden_state[:,0,:].unsqueeze(1), n_gram_feats, last_hidden_state[:,MAX_LEN-1,:].unsqueeze(1)), dim=1)\n",
    "        n_gram_feats = torch.cat((torch.zeros((batch_size,1, 256)).to(device), n_gram_feats, torch.zeros((batch_size,1, 256)).to(device)),dim=1)\n",
    "        # last_hidden_state = torch.cat((last_hidden_state, n_gram_feats), dim=-1)\n",
    "\n",
    "        cls_emb = context_outputs[1].unsqueeze(1)\n",
    "        h_0 = torch.randn(2, batch_size, 384).to(device)\n",
    "        #cls_emb, _ = self.gru(cls_emb, h_0)\n",
    "        cls_emb, _ = self.gru(cls_emb, h_0)\n",
    "        cls_emb = cls_emb.repeat(1, last_hidden_state.shape[1], 1)\n",
    "\n",
    "\n",
    "        diff_feat = torch.abs(last_hidden_state-cls_emb)\n",
    "        mul_feat = last_hidden_state*cls_emb\n",
    "\n",
    "        last_hidden_state = torch.cat((last_hidden_state, cls_emb, diff_feat, mul_feat, n_gram_feats), dim=-1)\n",
    "\n",
    "        # h_1 = torch.randn(2, batch_size, 768).to(device)\n",
    "        # last_hidden_state, _ = self.last_gru(last_hidden_state, h_1)\n",
    "\n",
    "\n",
    "\n",
    "        # outputs:(batch_size, seq_len, ent_type_size*inner_dim*2)\n",
    "        outputs = self.dense(last_hidden_state)\n",
    "        outputs = torch.split(outputs, self.inner_dim * 2, dim=-1)\n",
    "        # outputs:(batch_size, seq_len, ent_type_size, inner_dim*2)\n",
    "        outputs = torch.stack(outputs, dim=-2)\n",
    "        # qw,kw:(batch_size, seq_len, ent_type_size, inner_dim)\n",
    "        qw, kw = outputs[..., :self.inner_dim], outputs[..., self.inner_dim:]  # TODO:修改为Linear获取？\n",
    "\n",
    "        if self.RoPE:\n",
    "            # pos_emb:(batch_size, seq_len, inner_dim)\n",
    "            pos_emb = self.sinusoidal_position_embedding(batch_size, seq_len, self.inner_dim)\n",
    "            # cos_pos,sin_pos: (batch_size, seq_len, 1, inner_dim)\n",
    "            cos_pos = pos_emb[..., None, 1::2].repeat_interleave(2, dim=-1)\n",
    "            sin_pos = pos_emb[..., None, ::2].repeat_interleave(2, dim=-1)\n",
    "            qw2 = torch.stack([-qw[..., 1::2], qw[..., ::2]], -1)\n",
    "            qw2 = qw2.reshape(qw.shape)\n",
    "            qw = qw * cos_pos + qw2 * sin_pos\n",
    "            kw2 = torch.stack([-kw[..., 1::2], kw[..., ::2]], -1)\n",
    "            kw2 = kw2.reshape(kw.shape)\n",
    "            kw = kw * cos_pos + kw2 * sin_pos\n",
    "\n",
    "        # logits:(batch_size, ent_type_size, seq_len, seq_len)\n",
    "        logits = torch.einsum('bmhd,bnhd->bhmn', qw, kw)\n",
    "\n",
    "        # padding mask\n",
    "        pad_mask = attention_mask.unsqueeze(1).unsqueeze(1).expand(batch_size, self.ent_type_size, seq_len, seq_len)\n",
    "        # pad_mask_h = attention_mask.unsqueeze(1).unsqueeze(-1).expand(batch_size, self.ent_type_size, seq_len, seq_len)\n",
    "        # pad_mask = pad_mask_v&pad_mask_h\n",
    "        logits = logits * pad_mask - (1 - pad_mask) * 1e12\n",
    "\n",
    "        # 排除下三角\n",
    "        mask = torch.tril(torch.ones_like(logits), -1)\n",
    "        logits = logits - mask * 1e12\n",
    "\n",
    "        return logits / self.inner_dim ** 0.5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model = GlobalPointer(encoder, len(Ent2id), 64)  # (encoder, ent_type_size, inner_dim)\n",
    "model = model.to(device)\n",
    "\n",
    "#----------------------------------加载伪标训练模型-----------------------------------\n",
    "#path = 'model/best_model(5w伪标训练).bin'\n",
    "#print('使用伪标训练模型：', path)\n",
    "#model.load_state_dict(torch.load(path))\n",
    "\n",
    "\n",
    "def global_pointer_f1_score(y_true, y_pred):\n",
    "    y_pred = torch.gt(y_pred, 0)\n",
    "    return torch.sum(y_true * y_pred).item(), torch.sum(y_true + y_pred).item()\n",
    "\n",
    "\n",
    "def validation_fn(model, dev_loader, loss_fn):\n",
    "    model.eval()\n",
    "    ema.apply_shadow()\n",
    "    total_loss = []\n",
    "    cnt = 0\n",
    "    total_f1_, total_precision_, total_recall_ = 0., 0., 0.\n",
    "    with torch.no_grad():\n",
    "        for token_id, at_mask, label_id, token_type_ids in dev_loader:\n",
    "            outputs = model(token_id.to(device), at_mask.to(device), token_type_ids.to(device))\n",
    "            loss = loss_fn(outputs, label_id.to(device))\n",
    "            total_loss.append(loss.item())\n",
    "            cnt += 1\n",
    "            f1, p, r = metrics.get_evaluate_fpr(outputs, label_id.to(device))\n",
    "            total_f1_ += f1\n",
    "            total_precision_ += p\n",
    "            total_recall_ += r\n",
    "        avg_f1 = total_f1_ / cnt\n",
    "        avg_precision = total_precision_ / cnt\n",
    "        avg_recall = total_recall_ / cnt\n",
    "        # t_loss = np.array(total_loss).mean()\n",
    "    # ema.restore()\n",
    "    return avg_f1, avg_precision, avg_recall\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, dev_loader, model_save_path='../outputs',\n",
    "                early_stop_epochs=2, is_fgm=True, is_pgd=False):\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    ########优化器 学习率\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "         {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "     ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    loss_fn = GlobalPointerCrossEntropy().to(device)\n",
    "    # optimizer_grouped_parameters.append({'params': [p for p in loss_fn.parameters()]})\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=LR, eps=1e-8)\n",
    "\n",
    "    best_vmetric = 0\n",
    "    if is_fgm:\n",
    "        fgm = FGM(module=model)\n",
    "\n",
    "    if is_pgd:\n",
    "        pgd = PGD(model=model)\n",
    "        K = 3\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = []\n",
    "        total_f1 = []\n",
    "        model.train()\n",
    "        bar = tqdm_notebook(train_loader)\n",
    "\n",
    "        for idx, (token_id, at_mask, label_id, token_type_ids) in enumerate(bar):\n",
    "            outputs = model(token_id.to(device), at_mask.to(device), token_type_ids.to(device))\n",
    "            loss = loss_fn(outputs, label_id.to(device))\n",
    "            total_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            bar.set_postfix(loss=loss.item())\n",
    "            if is_fgm:\n",
    "                fgm.attack()\n",
    "                outputs = model(token_id.to(device), at_mask.to(device), token_type_ids.to(device))\n",
    "                loss_fgm = loss_fn(outputs, label_id.to(device))\n",
    "                loss_fgm.backward()\n",
    "                fgm.restore()\n",
    "            if is_pgd:\n",
    "                pgd.backup_grad()\n",
    "                for t in range(K):\n",
    "                    pgd.attack(is_first_attack=(t == 0))\n",
    "                    if t != K - 1:\n",
    "                        model.zero_grad()\n",
    "                    else:\n",
    "                        pgd.restore_grad()\n",
    "                    outputs = model(token_id.to(device), at_mask.to(device), token_type_ids.to(device))\n",
    "                    loss_pgd = loss_fn(outputs, label_id.to(device))\n",
    "                    loss_pgd.backward()\n",
    "                pgd.restore()\n",
    "            f1 = metrics.get_sample_f1(outputs, label_id.to(device))\n",
    "            total_f1.append(f1.item())\n",
    "            if ((idx + 1) % 4) == 0:\n",
    "                # optimizer the net\n",
    "                optimizer.step()  # update parameters of net\n",
    "                # ema.update()\n",
    "                optimizer.zero_grad()  # reset gradient\n",
    "                ema.update()\n",
    "        ema.apply_shadow()\n",
    "        t_loss = np.array(total_loss).mean()\n",
    "        t_f1 = np.array(total_f1).mean()\n",
    "        avg_f1, avg_precision, avg_recall = validation_fn(model, dev_loader, loss_fn)\n",
    "        print('epoch:{},训练集损失t_loss:{:.6f},准确率pre:{:.6f},召回率:{:.6f},F1_eval:{:.6f}'.format(epoch, t_loss,\n",
    "                                                                                           avg_precision, avg_recall,\n",
    "                                                                                           avg_f1))\n",
    "\n",
    "        model_save_path = 'model/best_model(9:1).bin'\n",
    "        if avg_f1 > best_vmetric:\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            best_vmetric = avg_f1\n",
    "            no_improve = 0\n",
    "            print('improve save model!!!')\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "\n",
    "        if no_improve_epochs == early_stop_epochs:\n",
    "            print('no improve score !!! stop train !!!')\n",
    "            break\n",
    "        ema.restore()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/12 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6647efed368e42f597751a1519a1b6fc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 6.00 GiB total capacity; 4.00 GiB already allocated; 0 bytes free; 4.17 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\MACHEN~1\\AppData\\Local\\Temp/ipykernel_7824/1677394068.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mema\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mregister\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mmetrics\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mMetricsCalculator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0mtrain_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdev_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mearly_stop_epochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mearly_stop_epochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Users\\MACHEN~1\\AppData\\Local\\Temp/ipykernel_7824/1872146441.py\u001B[0m in \u001B[0;36mtrain_model\u001B[1;34m(model, train_loader, dev_loader, model_save_path, early_stop_epochs, is_fgm, is_pgd)\u001B[0m\n\u001B[0;32m     68\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0midx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mtoken_id\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mat_mask\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabel_id\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtoken_type_ids\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbar\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 70\u001B[1;33m             \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtoken_id\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mat_mask\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtoken_type_ids\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     71\u001B[0m             \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mloss_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabel_id\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     72\u001B[0m             \u001B[0mtotal_loss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\ML_ENVS\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1049\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1053\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Users\\MACHEN~1\\AppData\\Local\\Temp/ipykernel_7824/2157843563.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids)\u001B[0m\n\u001B[0;32m    243\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdevice\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minput_ids\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    244\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 245\u001B[1;33m         \u001B[0mcontext_outputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mencoder\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattention_mask\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtoken_type_ids\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    246\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    247\u001B[0m         \u001B[1;31m#         print(context_outputs[0].size())  (batch_size,max_len,hidden_size)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\ML_ENVS\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1049\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1053\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\打工\\竞赛\\GAIIC\\nezha.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001B[0m\n\u001B[0;32m    675\u001B[0m             \u001B[0mhead_mask\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mhead_mask\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    676\u001B[0m             \u001B[0mencoder_hidden_states\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mencoder_hidden_states\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 677\u001B[1;33m             \u001B[0mencoder_attention_mask\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mencoder_extended_attention_mask\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    678\u001B[0m         )\n\u001B[0;32m    679\u001B[0m         \u001B[0msequence_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mencoder_outputs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\ML_ENVS\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1049\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1053\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\打工\\竞赛\\GAIIC\\nezha.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001B[0m\n\u001B[0;32m    504\u001B[0m                 \u001B[0mall_hidden_states\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mall_hidden_states\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    505\u001B[0m             layer_outputs = layer_module(\n\u001B[1;32m--> 506\u001B[1;33m                 \u001B[0mhidden_states\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattention_mask\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhead_mask\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoder_hidden_states\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoder_attention_mask\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    507\u001B[0m             )\n\u001B[0;32m    508\u001B[0m             \u001B[0mhidden_states\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlayer_outputs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\ML_ENVS\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1049\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1053\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\打工\\竞赛\\GAIIC\\nezha.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001B[0m\n\u001B[0;32m    477\u001B[0m             \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0moutputs\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mcross_attention_outputs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m  \u001B[1;31m# add cross attentions if we output attention weights\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    478\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 479\u001B[1;33m         \u001B[0mintermediate_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mintermediate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mattention_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    480\u001B[0m         \u001B[0mlayer_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moutput\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mintermediate_output\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattention_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    481\u001B[0m         \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mlayer_output\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0moutputs\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\ML_ENVS\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1049\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1053\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\ML_ENVS\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states)\u001B[0m\n\u001B[0;32m    424\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhidden_states\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    425\u001B[0m         \u001B[0mhidden_states\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdense\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 426\u001B[1;33m         \u001B[0mhidden_states\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mintermediate_act_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    427\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mhidden_states\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    428\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\ML_ENVS\\lib\\site-packages\\torch\\nn\\functional.py\u001B[0m in \u001B[0;36mgelu\u001B[1;34m(input)\u001B[0m\n\u001B[0;32m   1553\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mhas_torch_function_unary\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1554\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mhandle_torch_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgelu\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1555\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_C\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_nn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgelu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1556\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 6.00 GiB total capacity; 4.00 GiB already allocated; 0 bytes free; 4.17 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "ema = EMA(model, 0.995)\n",
    "ema.register()\n",
    "metrics = MetricsCalculator()\n",
    "train_model(model, train_loader, dev_loader, early_stop_epochs=early_stop_epochs)\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('共用时:', (end - start) / 60, '分钟')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "GlobalPointer(\n  (encoder): NeZhaModel(\n    (embeddings): NeZhaEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): NeZhaEncoder(\n      (layer): ModuleList(\n        (0): NeZhaLayer(\n          (attention): NeZhaAttention(\n            (self): NeZhaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (relative_positions_encoding): RelativePositionsEncoding()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): NeZhaLayer(\n          (attention): NeZhaAttention(\n            (self): NeZhaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (relative_positions_encoding): RelativePositionsEncoding()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): NeZhaLayer(\n          (attention): NeZhaAttention(\n            (self): NeZhaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (relative_positions_encoding): RelativePositionsEncoding()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): NeZhaLayer(\n          (attention): NeZhaAttention(\n            (self): NeZhaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (relative_positions_encoding): RelativePositionsEncoding()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): NeZhaLayer(\n          (attention): NeZhaAttention(\n            (self): NeZhaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (relative_positions_encoding): RelativePositionsEncoding()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): NeZhaLayer(\n          (attention): NeZhaAttention(\n            (self): NeZhaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (relative_positions_encoding): RelativePositionsEncoding()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): NeZhaLayer(\n          (attention): NeZhaAttention(\n            (self): NeZhaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (relative_positions_encoding): RelativePositionsEncoding()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): NeZhaLayer(\n          (attention): NeZhaAttention(\n            (self): NeZhaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (relative_positions_encoding): RelativePositionsEncoding()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): NeZhaLayer(\n          (attention): NeZhaAttention(\n            (self): NeZhaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (relative_positions_encoding): RelativePositionsEncoding()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): NeZhaLayer(\n          (attention): NeZhaAttention(\n            (self): NeZhaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (relative_positions_encoding): RelativePositionsEncoding()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): NeZhaLayer(\n          (attention): NeZhaAttention(\n            (self): NeZhaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (relative_positions_encoding): RelativePositionsEncoding()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): NeZhaLayer(\n          (attention): NeZhaAttention(\n            (self): NeZhaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (relative_positions_encoding): RelativePositionsEncoding()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dense): Linear(in_features=3328, out_features=6784, bias=True)\n  (gru): GRU(768, 384, batch_first=True, bidirectional=True)\n  (n_gram_fc): Linear(in_features=768, out_features=256, bias=True)\n  (n_gram_tanh): Tanh()\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}