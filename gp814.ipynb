{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机种子： 1998\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from ark_nlp.model.ner.global_pointer_bert import Tokenizer\n",
    "from ark_nlp.model.ner.global_pointer_bert import Dataset as Dt\n",
    "import os\n",
    "import jieba\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import logging\n",
    "from nezha import NeZhaConfig, NeZhaModel, NeZhaForMaskedLM\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # GPU\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # 禁止hash随机化\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # True的话会自动寻找最适合当前配置的高效算法，来达到优化运行效率的问题。False保证实验结果可复现\n",
    "\n",
    "# 设置随机数种子\n",
    "RANDOM_SEED = 1998\n",
    "print('随机种子：', RANDOM_SEED)\n",
    "setup_seed(RANDOM_SEED)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size:  8\n",
      "./my_nezha_cn_base2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./my_nezha_cn_base2/ were not used when initializing NeZhaModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing NeZhaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NeZhaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of NeZhaModel were not initialized from the model checkpoint at ./my_nezha_cn_base2/ and are newly initialized: ['bert.pooler.dense.weight', 'bert.encoder.layer.6.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.5.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.7.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.0.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.9.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.10.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.11.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.4.attention.self.relative_positions_encoding.positions_encoding', 'bert.pooler.dense.bias', 'bert.encoder.layer.3.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.2.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.8.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.1.attention.self.relative_positions_encoding.positions_encoding']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "dropout_num = 0\n",
    "start = time.time()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAX_LEN = 128\n",
    "batch_size = 8\n",
    "print('batch_size: ', batch_size)\n",
    "EPOCHS = 6\n",
    "LR = 2e-5\n",
    "early_stop_epochs = 2\n",
    "model_path = './my_nezha_cn_base2/'\n",
    "model_save_path = 'model'\n",
    "print(model_path)\n",
    "\n",
    "\n",
    "\n",
    "config = NeZhaConfig.from_json_file(model_path + 'config.json')\n",
    "config.num_labels = 53\n",
    "\n",
    "# encoder = NeZhaForSequenceClassification(config)\n",
    "encoder = NeZhaModel.from_pretrained(model_path, config=config)\n",
    "\n",
    "import os\n",
    "from ark_nlp.factory.utils.conlleval import get_entity_bio\n",
    "\n",
    "datalist = []\n",
    "with open('./datasets/preliminary_contest_datasets/train_data/train.txt', 'r', encoding='utf-8') as f:\n",
    "# with open('./单模伪标数据/pseudo_10w(单模).txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    lines.append('\\n')\n",
    "\n",
    "    text = []\n",
    "    labels = []\n",
    "    label_set = set()\n",
    "\n",
    "    for line in lines:\n",
    "        if line == '\\n':\n",
    "            text = ''.join(text)\n",
    "            entity_labels = []\n",
    "            for _type, _start_idx, _end_idx in get_entity_bio(labels, id2label=None):\n",
    "                entity_labels.append({\n",
    "                    'start_idx': _start_idx,\n",
    "                    'end_idx': _end_idx,\n",
    "                    'type': _type,\n",
    "                    'entity': text[_start_idx: _end_idx + 1]\n",
    "                })\n",
    "\n",
    "            if text == '':\n",
    "                continue\n",
    "\n",
    "            datalist.append({\n",
    "                'text': text,\n",
    "                'label': entity_labels,\n",
    "                'BIO': labels\n",
    "            })\n",
    "\n",
    "            text = []\n",
    "            labels = []\n",
    "\n",
    "        elif line == '  O\\n':\n",
    "            text.append(' ')\n",
    "            labels.append('O')\n",
    "        else:\n",
    "            line = line.strip('\\n').split()\n",
    "            if len(line) == 1:\n",
    "                term = ' '\n",
    "                label = line[0]\n",
    "            else:\n",
    "                term, label = line\n",
    "            text.append(term)\n",
    "            label_set.add(label.split('-')[-1])\n",
    "            labels.append(label)\n",
    "\n",
    "# 这里随意分割了一下看指标，建议实际使用sklearn分割或者交叉验证\n",
    "#datalist =  datalist[:10000]\n",
    "# train_data_df = pd.DataFrame(datalist)\n",
    "# print('训练集大小：', len(train_data_df))\n",
    "# train_data_df['label'] = train_data_df['label'].apply(lambda x: str(x))\n",
    "#\n",
    "# dev_data_df = pd.DataFrame(datalist[-400:])\n",
    "# dev_data_df['label'] = dev_data_df['label'].apply(lambda x: str(x))\n",
    "\n",
    "all_data = pd.DataFrame(datalist)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "40000"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "all_data = all_data[:100]\n",
    "\n",
    "train_data_df, dev_data_df = train_test_split(all_data, test_size = 0.1, random_state=RANDOM_SEED)\n",
    "#train_data_df = pd.DataFrame(datalist)\n",
    "# print('训练集大小：', len(train_data_df))\n",
    "# train_data_df['label'] = train_data_df['label'].apply(lambda x: str(x))\n",
    "#\n",
    "#dev_data_df = pd.DataFrame(datalist[-400:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小： 90\n",
      "验证集大小： 10\n"
     ]
    }
   ],
   "source": [
    "train_data_df.index = list(range(len(train_data_df)))\n",
    "dev_data_df.index = list(range(len(dev_data_df)))\n",
    "train_data_df['label'] = train_data_df['label'].apply(lambda x: str(x))\n",
    "dev_data_df['label'] = dev_data_df['label'].apply(lambda x: str(x))\n",
    "\n",
    "print('训练集大小：', len(train_data_df))\n",
    "print('验证集大小：', len(dev_data_df))\n",
    "\n",
    "label_list = sorted(list(label_set))\n",
    "del text\n",
    "del label_set\n",
    "del labels\n",
    "train_dataset = Dt(train_data_df, categories=label_list)\n",
    "dev_dataset = Dt(dev_data_df, categories=label_list)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "ark_tokenizer = Tokenizer(vocab=tokenizer, max_seq_len=128)\n",
    "\n",
    "train_dataset.convert_to_ids(ark_tokenizer)\n",
    "dev_dataset.convert_to_ids(ark_tokenizer)\n",
    "\n",
    "\n",
    "Ent2id = train_dataset.cat2id  # 53\n",
    "id2Ent = train_dataset.id2cat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "53"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Ent2id)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len, ark_data):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.ark_data = ark_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        token_ids, at_mask = self.get_token_ids(row)\n",
    "        return torch.tensor(token_ids, dtype=torch.long), torch.tensor(at_mask, dtype=torch.long), torch.tensor(\n",
    "            self.ark_data[index]['label_ids'].to_dense()), torch.tensor(self.ark_data[index]['token_type_ids'],\n",
    "                                                                        dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # def get_token_ids(self, row):\n",
    "    #     sentence = row.text\n",
    "    #     tokens = self.tokenizer.tokenize(sentence)\n",
    "    #     tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "    #     print(tokens)\n",
    "    #     padding = [0] * (self.max_len - len(tokens))\n",
    "    #     at_mask = [1] * len(tokens)\n",
    "    #     token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    #     token_ids = token_ids + padding\n",
    "    #     at_mask = at_mask + padding\n",
    "    #     return token_ids, at_mask\n",
    "    def get_token_ids(self, row):\n",
    "        sentence = row.text\n",
    "        #tokens = self.tokenizer.tokenize(sentence)\n",
    " \n",
    "        token_list = list(sentence)\n",
    "        tokens = []\n",
    "        for i, word in enumerate(token_list):\n",
    "            if word == ' ' or word == '':\n",
    "                word = '-'\n",
    "            token = tokenizer.tokenize(word)\n",
    "            if len(token) > 1:\n",
    "                token = [tokenizer.unk_token]\n",
    "            tokens.extend(token)\n",
    " \n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "        print(tokens)\n",
    "        padding = [0] * (self.max_len - len(tokens))\n",
    "        at_mask = [1] * len(tokens)\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        token_ids = token_ids + padding\n",
    "        at_mask = at_mask + padding\n",
    "        return token_ids, at_mask\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        token_ids = torch.stack([x[0] for x in batch])\n",
    "        at_mask = torch.stack([x[1] for x in batch])\n",
    "        labels = torch.stack([x[2] for x in batch])\n",
    "        token_type_ids = torch.stack([x[3] for x in batch])\n",
    "        return token_ids, at_mask, labels.squeeze(), token_type_ids\n",
    "\n",
    "\n",
    "ner_train_dataset = Dataset(train_data_df, ark_tokenizer, MAX_LEN, train_dataset)\n",
    "ner_dev_dataset = Dataset(dev_data_df, ark_tokenizer, MAX_LEN, dev_dataset)\n",
    "\n",
    "train_loader = DataLoader(ner_train_dataset,  # 1250\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=ner_train_dataset.collate_fn)\n",
    "dev_loader = DataLoader(ner_dev_dataset,  # 13\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        collate_fn=ner_dev_dataset.collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'v', 'i', 'v', 'o', 'x', '9', 's', '手', '机', '壳', '新', '款', '创', '意', '布', '纹', '硅', '胶', '防', '摔', '滑', '防', '汗', 'x', '9', '车', '载', '磁', '吸', '支', '架', '挂', '绳', '鹿', '头', '猫', 'x', '9', 's', 'p', 'l', 'u', 's', '男', '女', '薄', '潮', '包', '边', 'x', 'x', '9', 'p', 'l', 'u', 's', '磁', '吸', '布', '纹', '(', '猫', '粉', '色', ')', '[SEP]']\n",
      "['[CLS]', 'a', '4', '铁', '皮', '文', '件', '柜', '办', '公', '室', '资', '料', '收', '纳', '钢', '制', '带', '锁', '小', '柜', '子', '玻', '璃', '抽', '屉', '矮', '储', '物', '档', '案', '柜', '-', '分', '五', '节', '-', '0', '.', '8', 'm', 'm', '[SEP]']\n",
      "['[CLS]', '活', '页', '党', '员', '学', '习', '笔', '记', '本', '替', '换', '芯', '6', '孔', '两', '学', '一', '做', '内', '芯', 'a', '5', '米', '黄', '道', '林', '纸', '记', '事', '本', '子', '-', '党', '员', '会', '议', '内', '芯', 'a', '5', '尺', '寸', '[SEP]']\n",
      "['[CLS]', '戴', '尔', '（', 'd', 'e', 'l', 'l', '）', '成', '就', '3', '6', '7', '0', '-', '九', '代', 'i', '7', '-', '9', '7', '0', '0', '八', '核', '-', '台', '式', '机', '电', '脑', '-', '商', '用', '办', '公', '-', '游', '戏', 'c', 'a', 'd', '设', '计', '制', '图', '-', '主', '机', '+', 'p', '2', '7', '1', '9', 'h', '显', '示', '器', '-', '2', '7', '英', '寸', '-', '可', '升', '降', '-', '8', 'g', '/', '1', 't', '+', '1', '2', '8', 'g', '/', '1', '0', '5', '0', 't', 'i', '-', '4', 'g', '/', '定', '制', '[SEP]']\n",
      "['[CLS]', '迷', '你', '可', '充', '电', '桌', '面', '吸', '尘', '器', '电', '动', '清', '理', '吸', '灰', '机', '学', '生', '用', '品', '铅', '笔', '吸', '橡', '皮', '擦', '屑', '渣', '削', '沫', '便', '携', '清', '洁', '神', '器', '书', '桌', '自', '动', '清', '扫', '-', '蓝', '色', '-', '礼', '盒', '三', '件', '套', '（', '送', '7', '0', '枚', '橡', '皮', '擦', '替', '芯', '）', '[SEP]']\n",
      "['[CLS]', '秋', '叶', '原', '-', 'h', 'd', 'm', 'i', '线', '2', '.', '0', '高', '清', '线', '4', 'k', '数', '据', '3', 'd', '电', '脑', '5', '电', '视', '连', '接', '1', '0', '信', '号', '1', '5', '机', '顶', '盒', 'p', 's', '4', '投', '影', '仪', '2', '0', '米', '加', '长', '延', '长', '-', '黑', '色', '-', 'd', 'h', '-', '5', '5', '0', '-', '1', '.', '5', '米', '[SEP]']\n",
      "['[CLS]', '六', '品', '堂', '作', '文', '本', '原', '稿', '纸', '方', '格', '纸', '小', '学', '生', '信', '纸', '8', '0', '0', '格', '申', '论', '格', '子', '纸', '作', '业', '考', '试', '专', '用', '高', '考', '文', '稿', '语', '文', '作', '文', '纸', '稿', '纸', '学', '生', '用', '-', '1', '0', '1', '2', '方', '格', '绿', '色', '考', '试', '作', '文', '纸', '(', '2', '0', '张', '/', '本', ')', '*', '5', '本', '装', '[SEP]']\n",
      "['[CLS]', '美', '菱', 'b', 'c', 'd', '1', '7', '5', 'c', 'h', 'a', '1', '7', '5', 'z', 'm', '2', '1', '7', '6', 'a', '1', '7', '6', 'a', 'n', '1', '7', '6', 'm', '2', '冰', '箱', '门', '封', '条', '密', '封', '条', '密', '封', '圈', 'b', 'c', 'd', '-', '1', '7', '5', 'z', 'm', '2', '上', '门', '白', '色', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "for sample in train_loader:\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([8, 53, 128, 128])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[2].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class FGM(object):\n",
    "    def __init__(self, module):\n",
    "        self.module = module\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(\n",
    "            self,\n",
    "            epsilon=1.,\n",
    "            emb_name='word_embeddings'\n",
    "    ):\n",
    "        for name, param in self.module.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0 and not torch.isnan(norm):\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(\n",
    "            self,\n",
    "            emb_name='word_embeddings'\n",
    "    ):\n",
    "        for name, param in self.module.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "class PGD():\n",
    "    def __init__(self, model, emb_name='word_embeddings', epsilon=1., alpha=0.3):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        self.model = model\n",
    "        self.emb_name = emb_name\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.emb_backup = {}\n",
    "        self.grad_backup = {}\n",
    "\n",
    "    def attack(self, is_first_attack=False):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and self.emb_name in name:\n",
    "                if is_first_attack:\n",
    "                    self.emb_backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0:\n",
    "                    r_at = self.alpha * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "                    param.data = self.project(name, param.data, self.epsilon)\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and self.emb_name in name:\n",
    "                assert name in self.emb_backup\n",
    "                param.data = self.emb_backup[name]\n",
    "        self.emb_backup = {}\n",
    "\n",
    "    def project(self, param_name, param_data, epsilon):\n",
    "        r = param_data - self.emb_backup[param_name]\n",
    "        if torch.norm(r) > epsilon:\n",
    "            r = epsilon * r / torch.norm(r)\n",
    "        return self.emb_backup[param_name] + r\n",
    "\n",
    "    def backup_grad(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                self.grad_backup[name] = param.grad.clone()\n",
    "\n",
    "    def restore_grad(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                param.grad = self.grad_backup[name]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class EMA():\n",
    "    def __init__(self, model, decay):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "\n",
    "    def register(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def apply_shadow(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                self.backup[name] = param.data\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class GlobalPointerCrossEntropy(nn.Module):\n",
    "    '''Multi-class Focal loss implementation'''\n",
    "\n",
    "    def __init__(self, ):\n",
    "        super(GlobalPointerCrossEntropy, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def multilabel_categorical_crossentropy(y_true, y_pred):\n",
    "        y_pred = (1 - 2 * y_true) * y_pred\n",
    "        y_pred_neg = y_pred - y_true * 1e12\n",
    "        y_pred_pos = y_pred - (1 - y_true) * 1e12\n",
    "        zeros = torch.zeros_like(y_pred[..., :1])\n",
    "        y_pred_neg = torch.cat([y_pred_neg, zeros], dim=-1)\n",
    "        y_pred_pos = torch.cat([y_pred_pos, zeros], dim=-1)\n",
    "        neg_loss = torch.logsumexp(y_pred_neg, dim=-1)\n",
    "        pos_loss = torch.logsumexp(y_pred_pos, dim=-1)\n",
    "\n",
    "        return neg_loss + pos_loss\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        logits: [N, C, L, L]\n",
    "        \"\"\"\n",
    "        bh = logits.shape[0] * logits.shape[1]\n",
    "        target = torch.reshape(target, (bh, -1))\n",
    "        logits = torch.reshape(logits, (bh, -1))\n",
    "        return torch.mean(GlobalPointerCrossEntropy.multilabel_categorical_crossentropy(target, logits))\n",
    "\n",
    "\n",
    "class MetricsCalculator(object):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_sample_f1(self, y_pred, y_true):\n",
    "        y_pred = torch.gt(y_pred, 0).float()\n",
    "        return 2 * torch.sum(y_true * y_pred) / torch.sum(y_true + y_pred)\n",
    "\n",
    "    def get_sample_precision(self, y_pred, y_true):\n",
    "        y_pred = torch.gt(y_pred, 0).float()\n",
    "        return torch.sum(y_pred[y_true == 1]) / (y_pred.sum() + 1)\n",
    "\n",
    "    def get_evaluate_fpr(self, y_pred, y_true):\n",
    "        y_pred = y_pred.cpu().numpy()\n",
    "        y_true = y_true.cpu().numpy()\n",
    "        pred = []\n",
    "        true = []\n",
    "        for b, l, start, end in zip(*np.where(y_pred > 0)):\n",
    "            pred.append((b, l, start, end))\n",
    "        for b, l, start, end in zip(*np.where(y_true > 0)):\n",
    "            true.append((b, l, start, end))\n",
    "\n",
    "        R = set(pred)\n",
    "        T = set(true)\n",
    "        X = len(R & T)\n",
    "        Y = len(R)\n",
    "        Z = len(T)\n",
    "        if Y != 0:\n",
    "            f1, precision, recall = 2 * X / (Y + Z), X / Y, X / Z\n",
    "        else:\n",
    "            f1, precision, recall = 2 * X / (Y + Z), 0, X / Z\n",
    "        return f1, precision, recall"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "class GlobalPointer(nn.Module):\n",
    "    def __init__(self, encoder, ent_type_size, inner_dim, RoPE=True):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.ent_type_size = ent_type_size\n",
    "        self.inner_dim = inner_dim\n",
    "        self.hidden_size = encoder.config.hidden_size\n",
    "        self.dense = nn.Linear(self.hidden_size*2+256, self.ent_type_size * self.inner_dim * 2)\n",
    "        self.gru = nn.GRU(input_size=768,\n",
    "                          hidden_size=384,\n",
    "                          num_layers=1,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=True)\n",
    "        self.n_gram_fc = nn.Linear(768, 256)\n",
    "        self.n_gram_tanh = nn.Tanh()\n",
    "\n",
    "        #\n",
    "        # self.fc2 = nn.Linear(768, 768)\n",
    "        # self.tanh2 = nn.Tanh()\n",
    "        # self.last_gru = nn.GRU(input_size=self.hidden_size*2+256,\n",
    "        #                   hidden_size=768,\n",
    "        #                   num_layers=1,\n",
    "        #                   batch_first=True,\n",
    "        #                   bidirectional=True)\n",
    "\n",
    "        self.RoPE = RoPE\n",
    "\n",
    "    def sinusoidal_position_embedding(self, batch_size, seq_len, output_dim):\n",
    "        position_ids = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(-1)\n",
    "\n",
    "        indices = torch.arange(0, output_dim // 2, dtype=torch.float)\n",
    "        indices = torch.pow(10000, -2 * indices / output_dim)\n",
    "        embeddings = position_ids * indices\n",
    "        embeddings = torch.stack([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)\n",
    "        embeddings = embeddings.repeat((batch_size, *([1] * len(embeddings.shape))))\n",
    "        embeddings = torch.reshape(embeddings, (batch_size, seq_len, output_dim))\n",
    "        embeddings = embeddings.to(self.device)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_ngram_feats(self, x, ngram_range=1):\n",
    "        # ngram_range = 1表示取前后token 1个\n",
    "        n_gram_feats = []\n",
    "        for idx, i in enumerate(x):\n",
    "            if idx - ngram_range < 0 and idx + ngram_range > len(x) - 1:\n",
    "                temp = list()\n",
    "                for tidx in range(idx-ngram_range, idx+ngram_range+1):\n",
    "                    temp.append(x[tidx])\n",
    "                n_gram_feats.append(temp)\n",
    "            elif idx - ngram_range < 0:\n",
    "                temp = list()\n",
    "                for tidx in range(0, idx+ngram_range+1):\n",
    "                    temp.append(x[tidx])\n",
    "                n_gram_feats.append(temp)\n",
    "            elif idx + ngram_range > len(x) - 1:\n",
    "                temp = list()\n",
    "                for tidx in range(idx-ngram_range, len(x)):\n",
    "                    temp.append(x[tidx])\n",
    "                n_gram_feats.append(temp)\n",
    "            else:\n",
    "                temp = list()\n",
    "                for tidx in range(idx-ngram_range, idx+ngram_range+1):\n",
    "                    temp.append(x[tidx])\n",
    "                n_gram_feats.append(temp)\n",
    "\n",
    "        return n_gram_feats\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        self.device = input_ids.device\n",
    "\n",
    "        context_outputs = self.encoder(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "        #         print(context_outputs[0].size())  (batch_size,max_len,hidden_size)\n",
    "        #         print(context_outputs[1].size())  (batch_size,hidden_size)\n",
    "        # last_hidden_state:(batch_size, seq_len, hidden_size)\n",
    "        last_hidden_state = context_outputs[0]\n",
    "        # last_hidden_state.shape = (bs, seq_len, 768)\n",
    "\n",
    "        batch_size = last_hidden_state.size()[0]\n",
    "        seq_len = last_hidden_state.size()[1]\n",
    "\n",
    "        #cls_emb = context_outputs[1].unsqueeze(1)\n",
    "        # cls_emb.shape = (bs, 1, 768)\n",
    "        #avg_pool = torch.mean(last_hidden_state[:,1:MAX_LEN-1,:],dim=1).unsqueeze(1)\n",
    "        #avg_pool = self.tanh1(self.fc1(avg_pool))\n",
    "\n",
    "        # avg_pool.shape = (bs, 1, 768)\n",
    "        #max_pool, _ = torch.max(last_hidden_state[:,1:MAX_LEN-1,:],dim=1)\n",
    "        #max_pool = max_pool.unsqueeze(1)\n",
    "        #max_pool = self.tanh2(self.fc2(max_pool))\n",
    "        # max_pool.shape = (bs, 1, 768)\n",
    "\n",
    "\n",
    "        #deep_feature = torch.cat((cls_emb, avg_pool, max_pool), dim=-1)\n",
    "        # deep_feature.shape = (bs, 1, 768*3)\n",
    "\n",
    "        #deep_feature = deep_feature.repeat(1, last_hidden_state.shape[1], 1)\n",
    "        # deep_feature.shape = (bs, seq_len, 768*3)\n",
    "\n",
    "\n",
    "        #last_hidden_state = torch.cat((last_hidden_state, deep_feature), dim=-1)\n",
    "        # last_hidden_state.shape = (bs, seq_len, 768*4)\n",
    "\n",
    "        #h_0 = self._init_hidden(batchs=batch_size)\n",
    "        #last_hidden_state, _ = self.gru(last_hidden_state, h_0)\n",
    "\n",
    "        # print('last_hidden_state: ', last_hidden_state.shape)\n",
    "\n",
    "        temp_last_hidden_state = last_hidden_state[:,1:MAX_LEN-1,:]\n",
    "        # temp_last_hidden_state.shape = (bs, MAX_LEN-2, 768)\n",
    "        # print('temp_last_hidden_state: ', temp_last_hidden_state.shape)\n",
    "        n_gram_feats_idx = self.get_ngram_feats(list(range(temp_last_hidden_state.shape[1])), ngram_range=2)\n",
    "        n_gram_feats = []\n",
    "\n",
    "        #for i, n_gram in enumerate(n_gram_feats_idx):\n",
    "            #if i == 0:\n",
    "             #   n_gram_feats.append(torch.cat((torch.zeros((batch_size, 768)).to(device), temp_last_hidden_state[:, n_gram[0], :], temp_last_hidden_state[:, n_gram[1], :]), dim=-1))\n",
    "           # elif i == len(n_gram_feats_idx) - 1:\n",
    "            #    n_gram_feats.append(torch.cat((temp_last_hidden_state[:, n_gram[0], :], temp_last_hidden_state[:, n_gram[1], :], torch.zeros((batch_size, 768)).to(device)), dim=-1))\n",
    "           # else:\n",
    "            #    n_gram_feats.append(torch.cat((temp_last_hidden_state[:, n_gram[0], :], temp_last_hidden_state[:, n_gram[1], :], temp_last_hidden_state[:, n_gram[2], :]), dim=-1))\n",
    "\n",
    "       # n_gram_feats = torch.stack(n_gram_feats, dim=1)\n",
    "        # n_gram_feats.shape = (bs, MAX_LEN-2, 768*3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for n_gram in n_gram_feats_idx:\n",
    "\n",
    "            temp = temp_last_hidden_state[:, n_gram[0], :]\n",
    "            for i in range(1, len(n_gram)):\n",
    "                # temp += temp_last_hidden_state[:, n_gram[i], :]\n",
    "                temp = torch.add(temp, temp_last_hidden_state[:, n_gram[i], :])\n",
    "            n_gram_feats.append(temp)\n",
    "\n",
    "\n",
    "        n_gram_feats = torch.stack(n_gram_feats, dim=1)\n",
    "\n",
    "\n",
    "        n_gram_feats = self.n_gram_tanh(self.n_gram_fc(n_gram_feats))\n",
    "\n",
    "        # (bs, MAX_LEN-2, 256)\n",
    "\n",
    "\n",
    "        # last_hidden_state = torch.cat((last_hidden_state[:,0,:].unsqueeze(1), n_gram_feats, last_hidden_state[:,MAX_LEN-1,:].unsqueeze(1)), dim=1)\n",
    "        n_gram_feats = torch.cat((torch.zeros((batch_size,1, 256)).to(device), n_gram_feats, torch.zeros((batch_size,1, 256)).to(device)),dim=1)\n",
    "        last_hidden_state = torch.cat((last_hidden_state, n_gram_feats), dim=-1)\n",
    "\n",
    "        cls_emb = context_outputs[1].unsqueeze(1)\n",
    "        h_0 = torch.randn(2, batch_size, 384).to(device)\n",
    "        #cls_emb, _ = self.gru(cls_emb, h_0)\n",
    "        cls_emb, _ = self.gru(cls_emb, h_0)\n",
    "        cls_emb = cls_emb.repeat(1, last_hidden_state.shape[1], 1)\n",
    "        last_hidden_state = torch.cat((last_hidden_state, cls_emb), dim=-1)\n",
    "\n",
    "\n",
    "        # h_1 = torch.randn(2, batch_size, 768).to(device)\n",
    "        # last_hidden_state, _ = self.last_gru(last_hidden_state, h_1)\n",
    "\n",
    "\n",
    "\n",
    "        # outputs:(batch_size, seq_len, ent_type_size*inner_dim*2)\n",
    "        outputs = self.dense(last_hidden_state)\n",
    "        outputs = torch.split(outputs, self.inner_dim * 2, dim=-1)\n",
    "        # outputs:(batch_size, seq_len, ent_type_size, inner_dim*2)\n",
    "        outputs = torch.stack(outputs, dim=-2)\n",
    "        # qw,kw:(batch_size, seq_len, ent_type_size, inner_dim)\n",
    "        qw, kw = outputs[..., :self.inner_dim], outputs[..., self.inner_dim:]  # TODO:修改为Linear获取？\n",
    "\n",
    "        if self.RoPE:\n",
    "            # pos_emb:(batch_size, seq_len, inner_dim)\n",
    "            pos_emb = self.sinusoidal_position_embedding(batch_size, seq_len, self.inner_dim)\n",
    "            # cos_pos,sin_pos: (batch_size, seq_len, 1, inner_dim)\n",
    "            cos_pos = pos_emb[..., None, 1::2].repeat_interleave(2, dim=-1)\n",
    "            sin_pos = pos_emb[..., None, ::2].repeat_interleave(2, dim=-1)\n",
    "            qw2 = torch.stack([-qw[..., 1::2], qw[..., ::2]], -1)\n",
    "            qw2 = qw2.reshape(qw.shape)\n",
    "            qw = qw * cos_pos + qw2 * sin_pos\n",
    "            kw2 = torch.stack([-kw[..., 1::2], kw[..., ::2]], -1)\n",
    "            kw2 = kw2.reshape(kw.shape)\n",
    "            kw = kw * cos_pos + kw2 * sin_pos\n",
    "\n",
    "        # logits:(batch_size, ent_type_size, seq_len, seq_len)\n",
    "        logits = torch.einsum('bmhd,bnhd->bhmn', qw, kw)\n",
    "\n",
    "        # padding mask\n",
    "        pad_mask = attention_mask.unsqueeze(1).unsqueeze(1).expand(batch_size, self.ent_type_size, seq_len, seq_len)\n",
    "        # pad_mask_h = attention_mask.unsqueeze(1).unsqueeze(-1).expand(batch_size, self.ent_type_size, seq_len, seq_len)\n",
    "        # pad_mask = pad_mask_v&pad_mask_h\n",
    "        logits = logits * pad_mask - (1 - pad_mask) * 1e12\n",
    "\n",
    "        # 排除下三角\n",
    "        mask = torch.tril(torch.ones_like(logits), -1)\n",
    "        logits = logits - mask * 1e12\n",
    "\n",
    "        return logits / self.inner_dim ** 0.5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "model = GlobalPointer(encoder, len(Ent2id), 64)  # (encoder, ent_type_size, inner_dim)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.embeddings.word_embeddings.weight\n",
      "encoder.embeddings.token_type_embeddings.weight\n",
      "encoder.embeddings.LayerNorm.weight\n",
      "encoder.embeddings.LayerNorm.bias\n",
      "encoder.encoder.layer.0.attention.self.query.weight\n",
      "encoder.encoder.layer.0.attention.self.query.bias\n",
      "encoder.encoder.layer.0.attention.self.key.weight\n",
      "encoder.encoder.layer.0.attention.self.key.bias\n",
      "encoder.encoder.layer.0.attention.self.value.weight\n",
      "encoder.encoder.layer.0.attention.self.value.bias\n",
      "encoder.encoder.layer.0.attention.output.dense.weight\n",
      "encoder.encoder.layer.0.attention.output.dense.bias\n",
      "encoder.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.0.intermediate.dense.weight\n",
      "encoder.encoder.layer.0.intermediate.dense.bias\n",
      "encoder.encoder.layer.0.output.dense.weight\n",
      "encoder.encoder.layer.0.output.dense.bias\n",
      "encoder.encoder.layer.0.output.LayerNorm.weight\n",
      "encoder.encoder.layer.0.output.LayerNorm.bias\n",
      "encoder.encoder.layer.1.attention.self.query.weight\n",
      "encoder.encoder.layer.1.attention.self.query.bias\n",
      "encoder.encoder.layer.1.attention.self.key.weight\n",
      "encoder.encoder.layer.1.attention.self.key.bias\n",
      "encoder.encoder.layer.1.attention.self.value.weight\n",
      "encoder.encoder.layer.1.attention.self.value.bias\n",
      "encoder.encoder.layer.1.attention.output.dense.weight\n",
      "encoder.encoder.layer.1.attention.output.dense.bias\n",
      "encoder.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.1.intermediate.dense.weight\n",
      "encoder.encoder.layer.1.intermediate.dense.bias\n",
      "encoder.encoder.layer.1.output.dense.weight\n",
      "encoder.encoder.layer.1.output.dense.bias\n",
      "encoder.encoder.layer.1.output.LayerNorm.weight\n",
      "encoder.encoder.layer.1.output.LayerNorm.bias\n",
      "encoder.encoder.layer.2.attention.self.query.weight\n",
      "encoder.encoder.layer.2.attention.self.query.bias\n",
      "encoder.encoder.layer.2.attention.self.key.weight\n",
      "encoder.encoder.layer.2.attention.self.key.bias\n",
      "encoder.encoder.layer.2.attention.self.value.weight\n",
      "encoder.encoder.layer.2.attention.self.value.bias\n",
      "encoder.encoder.layer.2.attention.output.dense.weight\n",
      "encoder.encoder.layer.2.attention.output.dense.bias\n",
      "encoder.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.2.intermediate.dense.weight\n",
      "encoder.encoder.layer.2.intermediate.dense.bias\n",
      "encoder.encoder.layer.2.output.dense.weight\n",
      "encoder.encoder.layer.2.output.dense.bias\n",
      "encoder.encoder.layer.2.output.LayerNorm.weight\n",
      "encoder.encoder.layer.2.output.LayerNorm.bias\n",
      "encoder.encoder.layer.3.attention.self.query.weight\n",
      "encoder.encoder.layer.3.attention.self.query.bias\n",
      "encoder.encoder.layer.3.attention.self.key.weight\n",
      "encoder.encoder.layer.3.attention.self.key.bias\n",
      "encoder.encoder.layer.3.attention.self.value.weight\n",
      "encoder.encoder.layer.3.attention.self.value.bias\n",
      "encoder.encoder.layer.3.attention.output.dense.weight\n",
      "encoder.encoder.layer.3.attention.output.dense.bias\n",
      "encoder.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.3.intermediate.dense.weight\n",
      "encoder.encoder.layer.3.intermediate.dense.bias\n",
      "encoder.encoder.layer.3.output.dense.weight\n",
      "encoder.encoder.layer.3.output.dense.bias\n",
      "encoder.encoder.layer.3.output.LayerNorm.weight\n",
      "encoder.encoder.layer.3.output.LayerNorm.bias\n",
      "encoder.encoder.layer.4.attention.self.query.weight\n",
      "encoder.encoder.layer.4.attention.self.query.bias\n",
      "encoder.encoder.layer.4.attention.self.key.weight\n",
      "encoder.encoder.layer.4.attention.self.key.bias\n",
      "encoder.encoder.layer.4.attention.self.value.weight\n",
      "encoder.encoder.layer.4.attention.self.value.bias\n",
      "encoder.encoder.layer.4.attention.output.dense.weight\n",
      "encoder.encoder.layer.4.attention.output.dense.bias\n",
      "encoder.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.4.intermediate.dense.weight\n",
      "encoder.encoder.layer.4.intermediate.dense.bias\n",
      "encoder.encoder.layer.4.output.dense.weight\n",
      "encoder.encoder.layer.4.output.dense.bias\n",
      "encoder.encoder.layer.4.output.LayerNorm.weight\n",
      "encoder.encoder.layer.4.output.LayerNorm.bias\n",
      "encoder.encoder.layer.5.attention.self.query.weight\n",
      "encoder.encoder.layer.5.attention.self.query.bias\n",
      "encoder.encoder.layer.5.attention.self.key.weight\n",
      "encoder.encoder.layer.5.attention.self.key.bias\n",
      "encoder.encoder.layer.5.attention.self.value.weight\n",
      "encoder.encoder.layer.5.attention.self.value.bias\n",
      "encoder.encoder.layer.5.attention.output.dense.weight\n",
      "encoder.encoder.layer.5.attention.output.dense.bias\n",
      "encoder.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.5.intermediate.dense.weight\n",
      "encoder.encoder.layer.5.intermediate.dense.bias\n",
      "encoder.encoder.layer.5.output.dense.weight\n",
      "encoder.encoder.layer.5.output.dense.bias\n",
      "encoder.encoder.layer.5.output.LayerNorm.weight\n",
      "encoder.encoder.layer.5.output.LayerNorm.bias\n",
      "encoder.encoder.layer.6.attention.self.query.weight\n",
      "encoder.encoder.layer.6.attention.self.query.bias\n",
      "encoder.encoder.layer.6.attention.self.key.weight\n",
      "encoder.encoder.layer.6.attention.self.key.bias\n",
      "encoder.encoder.layer.6.attention.self.value.weight\n",
      "encoder.encoder.layer.6.attention.self.value.bias\n",
      "encoder.encoder.layer.6.attention.output.dense.weight\n",
      "encoder.encoder.layer.6.attention.output.dense.bias\n",
      "encoder.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.6.intermediate.dense.weight\n",
      "encoder.encoder.layer.6.intermediate.dense.bias\n",
      "encoder.encoder.layer.6.output.dense.weight\n",
      "encoder.encoder.layer.6.output.dense.bias\n",
      "encoder.encoder.layer.6.output.LayerNorm.weight\n",
      "encoder.encoder.layer.6.output.LayerNorm.bias\n",
      "encoder.encoder.layer.7.attention.self.query.weight\n",
      "encoder.encoder.layer.7.attention.self.query.bias\n",
      "encoder.encoder.layer.7.attention.self.key.weight\n",
      "encoder.encoder.layer.7.attention.self.key.bias\n",
      "encoder.encoder.layer.7.attention.self.value.weight\n",
      "encoder.encoder.layer.7.attention.self.value.bias\n",
      "encoder.encoder.layer.7.attention.output.dense.weight\n",
      "encoder.encoder.layer.7.attention.output.dense.bias\n",
      "encoder.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.7.intermediate.dense.weight\n",
      "encoder.encoder.layer.7.intermediate.dense.bias\n",
      "encoder.encoder.layer.7.output.dense.weight\n",
      "encoder.encoder.layer.7.output.dense.bias\n",
      "encoder.encoder.layer.7.output.LayerNorm.weight\n",
      "encoder.encoder.layer.7.output.LayerNorm.bias\n",
      "encoder.encoder.layer.8.attention.self.query.weight\n",
      "encoder.encoder.layer.8.attention.self.query.bias\n",
      "encoder.encoder.layer.8.attention.self.key.weight\n",
      "encoder.encoder.layer.8.attention.self.key.bias\n",
      "encoder.encoder.layer.8.attention.self.value.weight\n",
      "encoder.encoder.layer.8.attention.self.value.bias\n",
      "encoder.encoder.layer.8.attention.output.dense.weight\n",
      "encoder.encoder.layer.8.attention.output.dense.bias\n",
      "encoder.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.8.intermediate.dense.weight\n",
      "encoder.encoder.layer.8.intermediate.dense.bias\n",
      "encoder.encoder.layer.8.output.dense.weight\n",
      "encoder.encoder.layer.8.output.dense.bias\n",
      "encoder.encoder.layer.8.output.LayerNorm.weight\n",
      "encoder.encoder.layer.8.output.LayerNorm.bias\n",
      "encoder.encoder.layer.9.attention.self.query.weight\n",
      "encoder.encoder.layer.9.attention.self.query.bias\n",
      "encoder.encoder.layer.9.attention.self.key.weight\n",
      "encoder.encoder.layer.9.attention.self.key.bias\n",
      "encoder.encoder.layer.9.attention.self.value.weight\n",
      "encoder.encoder.layer.9.attention.self.value.bias\n",
      "encoder.encoder.layer.9.attention.output.dense.weight\n",
      "encoder.encoder.layer.9.attention.output.dense.bias\n",
      "encoder.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.9.intermediate.dense.weight\n",
      "encoder.encoder.layer.9.intermediate.dense.bias\n",
      "encoder.encoder.layer.9.output.dense.weight\n",
      "encoder.encoder.layer.9.output.dense.bias\n",
      "encoder.encoder.layer.9.output.LayerNorm.weight\n",
      "encoder.encoder.layer.9.output.LayerNorm.bias\n",
      "encoder.encoder.layer.10.attention.self.query.weight\n",
      "encoder.encoder.layer.10.attention.self.query.bias\n",
      "encoder.encoder.layer.10.attention.self.key.weight\n",
      "encoder.encoder.layer.10.attention.self.key.bias\n",
      "encoder.encoder.layer.10.attention.self.value.weight\n",
      "encoder.encoder.layer.10.attention.self.value.bias\n",
      "encoder.encoder.layer.10.attention.output.dense.weight\n",
      "encoder.encoder.layer.10.attention.output.dense.bias\n",
      "encoder.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.10.intermediate.dense.weight\n",
      "encoder.encoder.layer.10.intermediate.dense.bias\n",
      "encoder.encoder.layer.10.output.dense.weight\n",
      "encoder.encoder.layer.10.output.dense.bias\n",
      "encoder.encoder.layer.10.output.LayerNorm.weight\n",
      "encoder.encoder.layer.10.output.LayerNorm.bias\n",
      "encoder.encoder.layer.11.attention.self.query.weight\n",
      "encoder.encoder.layer.11.attention.self.query.bias\n",
      "encoder.encoder.layer.11.attention.self.key.weight\n",
      "encoder.encoder.layer.11.attention.self.key.bias\n",
      "encoder.encoder.layer.11.attention.self.value.weight\n",
      "encoder.encoder.layer.11.attention.self.value.bias\n",
      "encoder.encoder.layer.11.attention.output.dense.weight\n",
      "encoder.encoder.layer.11.attention.output.dense.bias\n",
      "encoder.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "encoder.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "encoder.encoder.layer.11.intermediate.dense.weight\n",
      "encoder.encoder.layer.11.intermediate.dense.bias\n",
      "encoder.encoder.layer.11.output.dense.weight\n",
      "encoder.encoder.layer.11.output.dense.bias\n",
      "encoder.encoder.layer.11.output.LayerNorm.weight\n",
      "encoder.encoder.layer.11.output.LayerNorm.bias\n",
      "encoder.pooler.dense.weight\n",
      "encoder.pooler.dense.bias\n",
      "dense.weight\n",
      "dense.bias\n",
      "gru.weight_ih_l0\n",
      "gru.weight_hh_l0\n",
      "gru.bias_ih_l0\n",
      "gru.bias_hh_l0\n",
      "gru.weight_ih_l0_reverse\n",
      "gru.weight_hh_l0_reverse\n",
      "gru.bias_ih_l0_reverse\n",
      "gru.bias_hh_l0_reverse\n",
      "n_gram_fc.weight\n",
      "n_gram_fc.bias\n"
     ]
    }
   ],
   "source": [
    "for n,p in model.named_parameters():\n",
    "    print(n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "GlobalPointer(\n  (encoder): NeZhaModel(\n    (embeddings): NeZhaEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0, inplace=False)\n    )\n    (encoder): NeZhaEncoder(\n      (layer): ModuleList(\n        (0): NeZhaLayer(\n          (attention): NeZhaAttention(\n            (self): NeZhaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (relative_positions_encoding): RelativePositionsEncoding()\n              (dropout): Dropout(p=0, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0, inplace=False)\n          )\n        )\n        (1): NeZhaLayer(\n          (attention): NeZhaAttention(\n            (self): NeZhaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (relative_positions_encoding): RelativePositionsEncoding()\n              (dropout): Dropout(p=0, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0, inplace=False)\n          )\n        )\n        (2): NeZhaLayer(\n          (attention): NeZhaAttention(\n            (self): NeZhaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (relative_positions_encoding): RelativePositionsEncoding()\n              (dropout): Dropout(p=0, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0, inplace=False)\n          )\n        )\n        (3): NeZhaLayer(\n          (attention): NeZhaAttention(\n            (self): NeZhaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (relative_positions_encoding): RelativePositionsEncoding()\n              (dropout): Dropout(p=0, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0, inplace=False)\n          )\n        )\n        (4): NeZhaLayer(\n          (attention): NeZhaAttention(\n            (self): NeZhaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (relative_positions_encoding): RelativePositionsEncoding()\n              (dropout): Dropout(p=0, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0, inplace=False)\n          )\n        )\n        (5): NeZhaLayer(\n          (attention): NeZhaAttention(\n            (self): NeZhaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (relative_positions_encoding): RelativePositionsEncoding()\n              (dropout): Dropout(p=0, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0, inplace=False)\n          )\n        )\n        (6): NeZhaLayer(\n          (attention): NeZhaAttention(\n            (self): NeZhaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (relative_positions_encoding): RelativePositionsEncoding()\n              (dropout): Dropout(p=0, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0, inplace=False)\n          )\n        )\n        (7): NeZhaLayer(\n          (attention): NeZhaAttention(\n            (self): NeZhaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (relative_positions_encoding): RelativePositionsEncoding()\n              (dropout): Dropout(p=0, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0, inplace=False)\n          )\n        )\n        (8): NeZhaLayer(\n          (attention): NeZhaAttention(\n            (self): NeZhaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (relative_positions_encoding): RelativePositionsEncoding()\n              (dropout): Dropout(p=0, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0, inplace=False)\n          )\n        )\n        (9): NeZhaLayer(\n          (attention): NeZhaAttention(\n            (self): NeZhaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (relative_positions_encoding): RelativePositionsEncoding()\n              (dropout): Dropout(p=0, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0, inplace=False)\n          )\n        )\n        (10): NeZhaLayer(\n          (attention): NeZhaAttention(\n            (self): NeZhaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (relative_positions_encoding): RelativePositionsEncoding()\n              (dropout): Dropout(p=0, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0, inplace=False)\n          )\n        )\n        (11): NeZhaLayer(\n          (attention): NeZhaAttention(\n            (self): NeZhaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (relative_positions_encoding): RelativePositionsEncoding()\n              (dropout): Dropout(p=0, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dense): Linear(in_features=1792, out_features=6784, bias=True)\n  (gru): GRU(768, 384, batch_first=True, bidirectional=True)\n  (n_gram_fc): Linear(in_features=768, out_features=256, bias=True)\n  (n_gram_tanh): Tanh()\n)"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def global_pointer_f1_score(y_true, y_pred):\n",
    "    y_pred = torch.gt(y_pred, 0)\n",
    "    return torch.sum(y_true * y_pred).item(), torch.sum(y_true + y_pred).item()\n",
    "\n",
    "\n",
    "def validation_fn(model, dev_loader, loss_fn):\n",
    "    model.eval()\n",
    "    ema.apply_shadow()\n",
    "    total_loss = []\n",
    "    cnt = 0\n",
    "    total_f1_, total_precision_, total_recall_ = 0., 0., 0.\n",
    "    with torch.no_grad():\n",
    "        for token_id, at_mask, label_id, token_type_ids in dev_loader:\n",
    "            outputs = model(token_id.to(device), at_mask.to(device), token_type_ids.to(device))\n",
    "            loss = loss_fn(outputs, label_id.to(device))\n",
    "            total_loss.append(loss.item())\n",
    "            cnt += 1\n",
    "            f1, p, r = metrics.get_evaluate_fpr(outputs, label_id.to(device))\n",
    "            total_f1_ += f1\n",
    "            total_precision_ += p\n",
    "            total_recall_ += r\n",
    "        avg_f1 = total_f1_ / cnt\n",
    "        avg_precision = total_precision_ / cnt\n",
    "        avg_recall = total_recall_ / cnt\n",
    "        # t_loss = np.array(total_loss).mean()\n",
    "    # ema.restore()\n",
    "    return avg_f1, avg_precision, avg_recall\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, dev_loader, model_save_path='../outputs',\n",
    "                early_stop_epochs=2, is_fgm=True, is_pgd=False):\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    ########优化器 学习率\n",
    "    # ---------------------------------------------模型层间差分学习率-------------------------------------------------------\n",
    "    # ---------------------------------------------模型层间差分学习率-------------------------------------------------------\n",
    "    def get_parameters(model, model_init_lr, multiplier, classifier_lr):\n",
    "        parameters = []\n",
    "        lr = model_init_lr\n",
    "        for layer in range(12,-1,-1):\n",
    "            layer_params = {\n",
    "                'params': [p for n,p in model.named_parameters() if f'encoder.layer.{layer}.' in n],\n",
    "                'lr': lr\n",
    "            }\n",
    "            parameters.append(layer_params)\n",
    "            lr *= multiplier\n",
    "        classifier_params = {\n",
    "            'params': [p for n,p in model.named_parameters() if 'linear' in n or 'pooling' in n \\\n",
    "                       or 'fc' in n or ('dense' in n and n[0]=='d') or 'gru' in n],\n",
    "            'lr': classifier_lr\n",
    "        }\n",
    "        parameters.append(classifier_params)\n",
    "\n",
    "        # 权重衰减\n",
    "        #no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        #decay_params = [\n",
    "        #{'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "         #{'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "        #]\n",
    "\n",
    "        #parameters.append(decay_params)\n",
    "\n",
    "        return parameters\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    parameters = get_parameters(model, 2e-5, 0.95, 1e-4)\n",
    "    optimizer = torch.optim.AdamW(parameters, lr=LR, eps=1e-8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    loss_fn = GlobalPointerCrossEntropy().to(device)\n",
    "    best_vmetric = 0\n",
    "    if is_fgm:\n",
    "        fgm = FGM(module=model)\n",
    "\n",
    "    if is_pgd:\n",
    "        pgd = PGD(model=model)\n",
    "        K = 3\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = []\n",
    "        total_f1 = []\n",
    "        model.train()\n",
    "        bar = tqdm_notebook(train_loader)\n",
    "\n",
    "        for idx, (token_id, at_mask, label_id, token_type_ids) in enumerate(bar):\n",
    "            outputs = model(token_id.to(device), at_mask.to(device), token_type_ids.to(device))\n",
    "            loss = loss_fn(outputs, label_id.to(device))\n",
    "            total_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            bar.set_postfix(loss=loss.item())\n",
    "            if is_fgm:\n",
    "                fgm.attack()\n",
    "                outputs = model(token_id.to(device), at_mask.to(device), token_type_ids.to(device))\n",
    "                loss_fgm = loss_fn(outputs, label_id.to(device))\n",
    "                loss_fgm.backward()\n",
    "                fgm.restore()\n",
    "            if is_pgd:\n",
    "                pgd.backup_grad()\n",
    "                for t in range(K):\n",
    "                    pgd.attack(is_first_attack=(t == 0))\n",
    "                    if t != K - 1:\n",
    "                        model.zero_grad()\n",
    "                    else:\n",
    "                        pgd.restore_grad()\n",
    "                    outputs = model(token_id.to(device), at_mask.to(device), token_type_ids.to(device))\n",
    "                    loss_pgd = loss_fn(outputs, label_id.to(device))\n",
    "                    loss_pgd.backward()\n",
    "                pgd.restore()\n",
    "            f1 = metrics.get_sample_f1(outputs, label_id.to(device))\n",
    "            total_f1.append(f1.item())\n",
    "            if ((idx + 1) % 4) == 0:\n",
    "                # optimizer the net\n",
    "                optimizer.step()  # update parameters of net\n",
    "                # ema.update()\n",
    "                optimizer.zero_grad()  # reset gradient\n",
    "                ema.update()\n",
    "        ema.apply_shadow()\n",
    "        t_loss = np.array(total_loss).mean()\n",
    "        t_f1 = np.array(total_f1).mean()\n",
    "        avg_f1, avg_precision, avg_recall = validation_fn(model, dev_loader, loss_fn)\n",
    "        print('epoch:{},训练集损失t_loss:{:.6f},准确率pre:{:.6f},召回率:{:.6f},F1_eval:{:.6f}'.format(epoch, t_loss,\n",
    "                                                                                           avg_precision, avg_recall,\n",
    "                                                                                           avg_f1))\n",
    "\n",
    "        model_save_path = 'model/nezha_train_model.bin'\n",
    "        if avg_f1 > best_vmetric:\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            best_vmetric = avg_f1\n",
    "            no_improve = 0\n",
    "            print('improve save model!!!')\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "\n",
    "        if no_improve_epochs == early_stop_epochs:\n",
    "            print('no improve score !!! stop train !!!')\n",
    "            break\n",
    "        ema.restore()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/12 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0fa3049e5e8f403e9172de7910fb0f54"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 6.00 GiB total capacity; 3.95 GiB already allocated; 0 bytes free; 4.15 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\MACHEN~1\\AppData\\Local\\Temp/ipykernel_4588/1861970377.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mema\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mregister\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mmetrics\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mMetricsCalculator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0mtrain_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdev_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mearly_stop_epochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mearly_stop_epochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Users\\MACHEN~1\\AppData\\Local\\Temp/ipykernel_4588/3921094784.py\u001B[0m in \u001B[0;36mtrain_model\u001B[1;34m(model, train_loader, dev_loader, model_save_path, early_stop_epochs, is_fgm, is_pgd)\u001B[0m\n\u001B[0;32m     91\u001B[0m             \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mloss_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabel_id\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     92\u001B[0m             \u001B[0mtotal_loss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 93\u001B[1;33m             \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     94\u001B[0m             \u001B[0mbar\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_postfix\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     95\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mis_fgm\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\ML_ENVS\\lib\\site-packages\\torch\\_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    253\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    254\u001B[0m                 inputs=inputs)\n\u001B[1;32m--> 255\u001B[1;33m         \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    256\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    257\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\ML_ENVS\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    147\u001B[0m     Variable._execution_engine.run_backward(\n\u001B[0;32m    148\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 149\u001B[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001B[0m\u001B[0;32m    150\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    151\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 6.00 GiB total capacity; 3.95 GiB already allocated; 0 bytes free; 4.15 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "ema = EMA(model, 0.995)\n",
    "ema.register()\n",
    "metrics = MetricsCalculator()\n",
    "train_model(model, train_loader, dev_loader, early_stop_epochs=early_stop_epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ark-nlp提供该函数：from ark_nlp.model.ner.global_pointer_bert import Predictor\n",
    "# 这里主要是为了可以比较清晰地看到解码过程，所以将代码copy到这\n",
    "class GlobalPointerNERPredictor(object):\n",
    "    \"\"\"\n",
    "    GlobalPointer命名实体识别的预测器\n",
    "\n",
    "    Args:\n",
    "        module: 深度学习模型\n",
    "        tokernizer: 分词器\n",
    "        cat2id (:obj:`dict`): 标签映射\n",
    "    \"\"\"  # noqa: ignore flake8\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            module,\n",
    "            tokernizer,\n",
    "            cat2id\n",
    "    ):\n",
    "        self.module = module\n",
    "        self.module.task = 'TokenLevel'\n",
    "\n",
    "        self.cat2id = cat2id\n",
    "        self.tokenizer = tokernizer\n",
    "        self.device = list(self.module.parameters())[0].device\n",
    "\n",
    "        self.id2cat = {}\n",
    "        for cat_, idx_ in self.cat2id.items():\n",
    "            self.id2cat[idx_] = cat_\n",
    "\n",
    "    def _convert_to_transfomer_ids(\n",
    "            self,\n",
    "            text\n",
    "    ):\n",
    "\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        token_mapping = self.tokenizer.get_token_mapping(text, tokens)\n",
    "\n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        padding = [0] * (self.tokenizer.max_seq_len - len(tokens))\n",
    "        segment_ids = segment_ids + padding\n",
    "        input_mask = [1] * len(tokens) + padding\n",
    "        input_ids = input_ids + padding\n",
    "\n",
    "        zero = [0 for i in range(self.tokenizer.max_seq_len)]\n",
    "        span_mask = [input_mask for i in range(sum(input_mask))]\n",
    "        span_mask.extend([zero for i in range(sum(input_mask), self.tokenizer.max_seq_len)])\n",
    "        span_mask = np.array(span_mask)\n",
    "\n",
    "        features = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': input_mask,\n",
    "            'token_type_ids': segment_ids,\n",
    "            'span_mask': span_mask\n",
    "        }\n",
    "\n",
    "        return features, token_mapping\n",
    "\n",
    "    def _get_input_ids(\n",
    "            self,\n",
    "            text\n",
    "    ):\n",
    "        if self.tokenizer.tokenizer_type == 'vanilla':\n",
    "            return self._convert_to_vanilla_ids(text)\n",
    "        elif self.tokenizer.tokenizer_type == 'transfomer':\n",
    "            return self._convert_to_transfomer_ids(text)\n",
    "        elif self.tokenizer.tokenizer_type == 'customized':\n",
    "            return self._convert_to_customized_ids(text)\n",
    "        else:\n",
    "            raise ValueError(\"The tokenizer type does not exist\")\n",
    "\n",
    "    def _get_module_one_sample_inputs(\n",
    "            self,\n",
    "            features\n",
    "    ):\n",
    "        return {col: torch.Tensor(features[col]).type(torch.long).unsqueeze(0).to(self.device) for col in features}\n",
    "\n",
    "    def predict_one_sample(\n",
    "            self,\n",
    "            text='',\n",
    "            threshold=0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        单样本预测\n",
    "\n",
    "        Args:\n",
    "            text (:obj:`string`): 输入文本\n",
    "            threshold (:obj:`float`, optional, defaults to 0): 预测的阈值\n",
    "        \"\"\"  # noqa: ignore flake8\"\n",
    "\n",
    "        features, token_mapping = self._get_input_ids(text)\n",
    "        self.module.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = self._get_module_one_sample_inputs(features)\n",
    "            input_ids = inputs['input_ids']\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            token_type_ids = inputs['token_type_ids']\n",
    "\n",
    "            scores = self.module(input_ids, attention_mask, token_type_ids)[0].cpu()\n",
    "\n",
    "        scores[:, [0, -1]] -= np.inf\n",
    "        scores[:, :, [0, -1]] -= np.inf\n",
    "\n",
    "        entities = []\n",
    "\n",
    "        for category, start, end in zip(*np.where(scores > threshold)):\n",
    "            if end - 1 > token_mapping[-1][-1]:\n",
    "                break\n",
    "            if token_mapping[start - 1][0] <= token_mapping[end - 1][-1]:\n",
    "                entitie_ = {\n",
    "                    \"start_idx\": token_mapping[start - 1][0],\n",
    "                    \"end_idx\": token_mapping[end - 1][-1],\n",
    "                    \"entity\": text[token_mapping[start - 1][0]: token_mapping[end - 1][-1] + 1],\n",
    "                    \"type\": self.id2cat[category]\n",
    "                }\n",
    "\n",
    "                if entitie_['entity'] == '':\n",
    "                    continue\n",
    "\n",
    "                entities.append(entitie_)\n",
    "\n",
    "        return entities\n",
    "\n",
    "\n",
    "# 加载最优模型\n",
    "print('*' * 20, 'Starting Predicting........', '*' * 20)\n",
    "encoder = NeZhaModel.from_pretrained(model_path, config=config)\n",
    "model = GlobalPointer(encoder, len(Ent2id), 64)  # (encoder, ent_type_size, inner_dim)\n",
    "model.to(device)\n",
    "path = 'model/nezha_train_model.bin'\n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()\n",
    "ner_predictor_instance = GlobalPointerNERPredictor(model, ark_tokenizer, Ent2id)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "predict_results = []\n",
    "\n",
    "with open('./datasets/preliminary_contest_datasets/preliminary_test_b/sample_per_line_preliminary_B.txt', 'r',\n",
    "          encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for _line in tqdm(lines):\n",
    "        label = len(_line) * ['O']\n",
    "        for _preditc in ner_predictor_instance.predict_one_sample(_line[:-1]):\n",
    "            if 'I' in label[_preditc['start_idx']]:\n",
    "                continue\n",
    "            if 'B' in label[_preditc['start_idx']] and 'O' not in label[_preditc['end_idx']]:\n",
    "                continue\n",
    "            if 'O' in label[_preditc['start_idx']] and 'B' in label[_preditc['end_idx']]:\n",
    "                continue\n",
    "\n",
    "            label[_preditc['start_idx']] = 'B-' + _preditc['type']\n",
    "            label[_preditc['start_idx'] + 1: _preditc['end_idx'] + 1] = (_preditc['end_idx'] - _preditc[\n",
    "                'start_idx']) * [('I-' + _preditc['type'])]\n",
    "\n",
    "        predict_results.append([_line, label])\n",
    "\n",
    "with open('outputs/global_pointer_baseline.txt', 'w', encoding='utf-8') as f:\n",
    "    for _result in predict_results:\n",
    "        for word, tag in zip(_result[0], _result[1]):\n",
    "            if word == '\\n':\n",
    "                continue\n",
    "            f.writelines(f'{word} {tag}\\n')\n",
    "        f.writelines('\\n')\n",
    "end = time.time()\n",
    "print('共用时:', (end - start) / 60, '分钟')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}