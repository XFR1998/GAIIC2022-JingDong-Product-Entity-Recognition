{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from ark_nlp.model.ner.global_pointer_bert import Tokenizer\n",
    "from ark_nlp.model.ner.global_pointer_bert import Dataset as Dt\n",
    "import os\n",
    "import jieba\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_warning()\n",
    "\n",
    "from nezha import NeZhaConfig, NeZhaModel, NeZhaForMaskedLM\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from ark_nlp.factory.utils.conlleval import get_entity_bio\n",
    "\n",
    "datalist = []\n",
    "with open('./datasets/preliminary_contest_datasets/train_data/train.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    lines.append('\\n')\n",
    "\n",
    "    text = []\n",
    "    labels = []\n",
    "    label_set = set()\n",
    "\n",
    "    for line in lines:\n",
    "        if line == '\\n':\n",
    "            text = ''.join(text)\n",
    "            entity_labels = []\n",
    "            for _type, _start_idx, _end_idx in get_entity_bio(labels, id2label=None):\n",
    "                entity_labels.append({\n",
    "                    'start_idx': _start_idx,\n",
    "                    'end_idx': _end_idx,\n",
    "                    'type': _type,\n",
    "                    'entity': text[_start_idx: _end_idx + 1]\n",
    "                })\n",
    "\n",
    "            if text == '':\n",
    "                continue\n",
    "\n",
    "            datalist.append({\n",
    "                'text': text,\n",
    "                'label': entity_labels,\n",
    "                'BIO': labels\n",
    "            })\n",
    "\n",
    "            text = []\n",
    "            labels = []\n",
    "\n",
    "        elif line == '  O\\n':\n",
    "            text.append(' ')\n",
    "            labels.append('O')\n",
    "        else:\n",
    "            line = line.strip('\\n').split()\n",
    "            if len(line) == 1:\n",
    "                term = ' '\n",
    "                label = line[0]\n",
    "            else:\n",
    "                term, label = line\n",
    "            text.append(term)\n",
    "            label_set.add(label.split('-')[-1])\n",
    "            labels.append(label)\n",
    "datalist = datalist\n",
    "# 这里随意分割了一下看指标，建议实际使用sklearn分割或者交叉验证\n",
    "train_data_df = pd.DataFrame(datalist)\n",
    "train_data_df['label'] = train_data_df['label'].apply(lambda x: str(x))\n",
    "\n",
    "# dev_data_df = pd.DataFrame(datalist[-400:])\n",
    "# dev_data_df['label'] = dev_data_df['label'].apply(lambda x: str(x))\n",
    "\n",
    "# train_data_df, dev_data_df = train_test_split(pd.DataFrame(datalist), test_size=0.1, random_state=42)\n",
    "# train_data_df['label'] = train_data_df['label'].apply(lambda x: str(x))\n",
    "# dev_data_df['label'] = dev_data_df['label'].apply(lambda x: str(x))\n",
    "\n",
    "label_list = sorted(list(label_set))\n",
    "\n",
    "train_dataset = Dt(train_data_df, categories=label_list)\n",
    "# dev_dataset = Dt(dev_data_df, categories=label_list)\n",
    "original_model_path = './my_nezha_cn_base/'\n",
    "tokenizer = BertTokenizer.from_pretrained(original_model_path)\n",
    "ark_tokenizer = Tokenizer(vocab=tokenizer, max_seq_len=128)\n",
    "\n",
    "train_dataset.convert_to_ids(ark_tokenizer)\n",
    "# dev_dataset.convert_to_ids(ark_tokenizer)\n",
    "\n",
    "train_labels_id = []\n",
    "for i in range(len(train_dataset)):\n",
    "    train_labels_id.append(train_dataset[i]['label_ids'])\n",
    "dev_labels_id = []\n",
    "# for i in range(len(dev_dataset)):\n",
    "#     dev_labels_id.append(dev_dataset[i]['label_ids'])\n",
    "Ent2id = train_dataset.cat2id  # 53\n",
    "id2Ent = train_dataset.id2cat\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "class GlobalPointerNERPredictor(object):\n",
    "    \"\"\"\n",
    "    GlobalPointer命名实体识别的预测器\n",
    "\n",
    "    Args:\n",
    "        module: 深度学习模型\n",
    "        tokernizer: 分词器\n",
    "        cat2id (:obj:`dict`): 标签映射\n",
    "    \"\"\"  # noqa: ignore flake8\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            module,\n",
    "            tokernizer,\n",
    "            cat2id\n",
    "    ):\n",
    "        self.module = module\n",
    "        self.module.task = 'TokenLevel'\n",
    "\n",
    "        self.cat2id = cat2id\n",
    "        self.tokenizer = tokernizer\n",
    "        self.device = list(self.module.parameters())[0].device\n",
    "\n",
    "        self.id2cat = {}\n",
    "        for cat_, idx_ in self.cat2id.items():\n",
    "            self.id2cat[idx_] = cat_\n",
    "\n",
    "    def _convert_to_transfomer_ids(\n",
    "            self,\n",
    "            text\n",
    "    ):\n",
    "\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        token_mapping = self.tokenizer.get_token_mapping(text, tokens)\n",
    "\n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        padding = [0] * (self.tokenizer.max_seq_len - len(tokens))\n",
    "        segment_ids = segment_ids + padding\n",
    "        input_mask = [1] * len(tokens) + padding\n",
    "        input_ids = input_ids + padding\n",
    "\n",
    "        zero = [0 for i in range(self.tokenizer.max_seq_len)]\n",
    "        span_mask = [input_mask for i in range(sum(input_mask))]\n",
    "        span_mask.extend([zero for i in range(sum(input_mask), self.tokenizer.max_seq_len)])\n",
    "        span_mask = np.array(span_mask)\n",
    "\n",
    "        features = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': input_mask,\n",
    "            'token_type_ids': segment_ids,\n",
    "            'span_mask': span_mask\n",
    "        }\n",
    "\n",
    "        return features, token_mapping\n",
    "\n",
    "    def _get_input_ids(\n",
    "            self,\n",
    "            text\n",
    "    ):\n",
    "        if self.tokenizer.tokenizer_type == 'vanilla':\n",
    "            return self._convert_to_vanilla_ids(text)\n",
    "        elif self.tokenizer.tokenizer_type == 'transfomer':\n",
    "            return self._convert_to_transfomer_ids(text)\n",
    "        elif self.tokenizer.tokenizer_type == 'customized':\n",
    "            return self._convert_to_customized_ids(text)\n",
    "        else:\n",
    "            raise ValueError(\"The tokenizer type does not exist\")\n",
    "\n",
    "    def _get_module_one_sample_inputs(\n",
    "            self,\n",
    "            features\n",
    "    ):\n",
    "        return {col: torch.Tensor(features[col]).type(torch.long).unsqueeze(0).to(self.device) for col in features}\n",
    "\n",
    "    def predict_one_sample(\n",
    "            self,\n",
    "            text='',\n",
    "            threshold=0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        单样本预测\n",
    "\n",
    "        Args:\n",
    "            text (:obj:`string`): 输入文本\n",
    "            threshold (:obj:`float`, optional, defaults to 0): 预测的阈值\n",
    "        \"\"\"  # noqa: ignore flake8\"\n",
    "\n",
    "        features, token_mapping = self._get_input_ids(text)\n",
    "        self.module.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = self._get_module_one_sample_inputs(features)\n",
    "            input_ids = inputs['input_ids']\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            token_type_ids = inputs['token_type_ids']\n",
    "\n",
    "            scores = self.module(input_ids, attention_mask, token_type_ids)[0].cpu()\n",
    "\n",
    "        scores[:, [0, -1]] -= np.inf\n",
    "        scores[:, :, [0, -1]] -= np.inf\n",
    "\n",
    "        entities = []\n",
    "\n",
    "        for category, start, end in zip(*np.where(scores > threshold)):\n",
    "            if end - 1 > token_mapping[-1][-1]:\n",
    "                break\n",
    "            if token_mapping[start - 1][0] <= token_mapping[end - 1][-1]:\n",
    "                entitie_ = {\n",
    "                    \"start_idx\": token_mapping[start - 1][0],\n",
    "                    \"end_idx\": token_mapping[end - 1][-1],\n",
    "                    \"entity\": text[token_mapping[start - 1][0]: token_mapping[end - 1][-1] + 1],\n",
    "                    \"type\": self.id2cat[category]\n",
    "                }\n",
    "\n",
    "                if entitie_['entity'] == '':\n",
    "                    continue\n",
    "\n",
    "                entities.append(entitie_)\n",
    "\n",
    "        return entities\n",
    "\n",
    "\n",
    "class GlobalPointer(nn.Module):\n",
    "    def __init__(self, encoder, ent_type_size, inner_dim, RoPE=True):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.ent_type_size = ent_type_size\n",
    "        self.inner_dim = inner_dim\n",
    "        self.hidden_size = encoder.config.hidden_size\n",
    "        self.dense = nn.Linear(self.hidden_size, self.ent_type_size * self.inner_dim * 2)\n",
    "        self.gru = nn.GRU(input_size=768,\n",
    "                          hidden_size=384,\n",
    "                          num_layers=1,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=True)\n",
    "\n",
    "        self.RoPE = RoPE\n",
    "\n",
    "    def sinusoidal_position_embedding(self, batch_size, seq_len, output_dim):\n",
    "        position_ids = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(-1)\n",
    "\n",
    "        indices = torch.arange(0, output_dim // 2, dtype=torch.float)\n",
    "        indices = torch.pow(10000, -2 * indices / output_dim)\n",
    "        embeddings = position_ids * indices\n",
    "        embeddings = torch.stack([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)\n",
    "        embeddings = embeddings.repeat((batch_size, *([1] * len(embeddings.shape))))\n",
    "        embeddings = torch.reshape(embeddings, (batch_size, seq_len, output_dim))\n",
    "        embeddings = embeddings.to(self.device)\n",
    "        return embeddings\n",
    "\n",
    "    def _init_hidden(self, batchs):\n",
    "        h_0 = torch.randn(2, batchs, 384).to(device)\n",
    "        return h_0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        self.device = input_ids.device\n",
    "\n",
    "        context_outputs = self.encoder(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "        #         print(context_outputs[0].size())  (batch_size,max_len,hidden_size)\n",
    "        #         print(context_outputs[1].size())  (batch_size,hidden_size)\n",
    "        # last_hidden_state:(batch_size, seq_len, hidden_size)\n",
    "        last_hidden_state = context_outputs[0]\n",
    "\n",
    "        batch_size = last_hidden_state.size()[0]\n",
    "        seq_len = last_hidden_state.size()[1]\n",
    "\n",
    "        #cls_emb = context_outputs[1].unsqueeze(1)\n",
    "        #h_0 = self._init_hidden(batchs=batch_size)\n",
    "        #cls_emb, _ = self.gru(cls_emb, h_0)\n",
    "        #cls_emb = cls_emb.repeat(1, last_hidden_state.shape[1], 1)\n",
    "        #last_hidden_state = torch.cat((last_hidden_state, cls_emb), dim=-1)\n",
    "\n",
    "        # outputs:(batch_size, seq_len, ent_type_size*inner_dim*2)\n",
    "        outputs = self.dense(last_hidden_state)\n",
    "        outputs = torch.split(outputs, self.inner_dim * 2, dim=-1)\n",
    "        # outputs:(batch_size, seq_len, ent_type_size, inner_dim*2)\n",
    "        outputs = torch.stack(outputs, dim=-2)\n",
    "        # qw,kw:(batch_size, seq_len, ent_type_size, inner_dim)\n",
    "        qw, kw = outputs[..., :self.inner_dim], outputs[..., self.inner_dim:]  # TODO:修改为Linear获取？\n",
    "\n",
    "        if self.RoPE:\n",
    "            # pos_emb:(batch_size, seq_len, inner_dim)\n",
    "            pos_emb = self.sinusoidal_position_embedding(batch_size, seq_len, self.inner_dim)\n",
    "            # cos_pos,sin_pos: (batch_size, seq_len, 1, inner_dim)\n",
    "            cos_pos = pos_emb[..., None, 1::2].repeat_interleave(2, dim=-1)\n",
    "            sin_pos = pos_emb[..., None, ::2].repeat_interleave(2, dim=-1)\n",
    "            qw2 = torch.stack([-qw[..., 1::2], qw[..., ::2]], -1)\n",
    "            qw2 = qw2.reshape(qw.shape)\n",
    "            qw = qw * cos_pos + qw2 * sin_pos\n",
    "            kw2 = torch.stack([-kw[..., 1::2], kw[..., ::2]], -1)\n",
    "            kw2 = kw2.reshape(kw.shape)\n",
    "            kw = kw * cos_pos + kw2 * sin_pos\n",
    "\n",
    "        # logits:(batch_size, ent_type_size, seq_len, seq_len)\n",
    "        logits = torch.einsum('bmhd,bnhd->bhmn', qw, kw)\n",
    "\n",
    "        # padding mask\n",
    "        pad_mask = attention_mask.unsqueeze(1).unsqueeze(1).expand(batch_size, self.ent_type_size, seq_len, seq_len)\n",
    "        # pad_mask_h = attention_mask.unsqueeze(1).unsqueeze(-1).expand(batch_size, self.ent_type_size, seq_len, seq_len)\n",
    "        # pad_mask = pad_mask_v&pad_mask_h\n",
    "        logits = logits * pad_mask - (1 - pad_mask) * 1e12\n",
    "\n",
    "        # 排除下三角\n",
    "        mask = torch.tril(torch.ones_like(logits), -1)\n",
    "        logits = logits - mask * 1e12\n",
    "\n",
    "        return logits / self.inner_dim ** 0.5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# KF = 5 代表你使用的是5折"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]Some weights of the model checkpoint at ./my_nezha_cn_base/ were not used when initializing NeZhaModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing NeZhaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NeZhaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of NeZhaModel were not initialized from the model checkpoint at ./my_nezha_cn_base/ and are newly initialized: ['bert.encoder.layer.7.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.11.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.1.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.3.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.6.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.4.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.9.attention.self.relative_positions_encoding.positions_encoding', 'bert.pooler.dense.weight', 'bert.encoder.layer.5.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.10.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.0.attention.self.relative_positions_encoding.positions_encoding', 'bert.pooler.dense.bias', 'bert.encoder.layer.2.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.8.attention.self.relative_positions_encoding.positions_encoding']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 20%|██        | 1/5 [00:06<00:25,  6.37s/it]Some weights of the model checkpoint at ./my_nezha_cn_base/ were not used when initializing NeZhaModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing NeZhaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NeZhaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of NeZhaModel were not initialized from the model checkpoint at ./my_nezha_cn_base/ and are newly initialized: ['bert.encoder.layer.7.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.11.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.1.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.3.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.6.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.4.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.9.attention.self.relative_positions_encoding.positions_encoding', 'bert.pooler.dense.weight', 'bert.encoder.layer.5.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.10.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.0.attention.self.relative_positions_encoding.positions_encoding', 'bert.pooler.dense.bias', 'bert.encoder.layer.2.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.8.attention.self.relative_positions_encoding.positions_encoding']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 40%|████      | 2/5 [00:09<00:12,  4.19s/it]Some weights of the model checkpoint at ./my_nezha_cn_base/ were not used when initializing NeZhaModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing NeZhaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NeZhaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of NeZhaModel were not initialized from the model checkpoint at ./my_nezha_cn_base/ and are newly initialized: ['bert.encoder.layer.7.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.11.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.1.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.3.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.6.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.4.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.9.attention.self.relative_positions_encoding.positions_encoding', 'bert.pooler.dense.weight', 'bert.encoder.layer.5.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.10.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.0.attention.self.relative_positions_encoding.positions_encoding', 'bert.pooler.dense.bias', 'bert.encoder.layer.2.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.8.attention.self.relative_positions_encoding.positions_encoding']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 60%|██████    | 3/5 [00:11<00:07,  3.52s/it]Some weights of the model checkpoint at ./my_nezha_cn_base/ were not used when initializing NeZhaModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing NeZhaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NeZhaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of NeZhaModel were not initialized from the model checkpoint at ./my_nezha_cn_base/ and are newly initialized: ['bert.encoder.layer.7.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.11.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.1.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.3.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.6.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.4.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.9.attention.self.relative_positions_encoding.positions_encoding', 'bert.pooler.dense.weight', 'bert.encoder.layer.5.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.10.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.0.attention.self.relative_positions_encoding.positions_encoding', 'bert.pooler.dense.bias', 'bert.encoder.layer.2.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.8.attention.self.relative_positions_encoding.positions_encoding']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 80%|████████  | 4/5 [00:14<00:03,  3.14s/it]Some weights of the model checkpoint at ./my_nezha_cn_base/ were not used when initializing NeZhaModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing NeZhaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NeZhaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of NeZhaModel were not initialized from the model checkpoint at ./my_nezha_cn_base/ and are newly initialized: ['bert.encoder.layer.7.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.11.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.1.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.3.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.6.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.4.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.9.attention.self.relative_positions_encoding.positions_encoding', 'bert.pooler.dense.weight', 'bert.encoder.layer.5.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.10.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.0.attention.self.relative_positions_encoding.positions_encoding', 'bert.pooler.dense.bias', 'bert.encoder.layer.2.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.8.attention.self.relative_positions_encoding.positions_encoding']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 5/5 [00:16<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1fold开始预测啦！！！\n",
      "2fold开始预测啦！！！\n",
      "3fold开始预测啦！！！\n",
      "4fold开始预测啦！！！\n",
      "5fold开始预测啦！！！\n"
     ]
    }
   ],
   "source": [
    "KF = 5\n",
    "# 5折模型\n",
    "for i in tqdm(range(1, KF+1)):\n",
    "    print('{}fold开始预测啦！！！'.format(i))\n",
    "    # i=3\n",
    "    config = NeZhaConfig.from_json_file(original_model_path + 'config.json')\n",
    "    config.num_labels = 53\n",
    "    encoder = NeZhaModel.from_pretrained(original_model_path, config=config)\n",
    "    model = GlobalPointer(encoder, len(Ent2id), 64)  # (encoder, ent_type_size, inner_dim)\n",
    "    model.to(device)\n",
    "    model_path = 'save_model/nezha_train_model_{}fold.bin'.format(i)\n",
    "    model.load_state_dict(torch.load(model_path,map_location=device))\n",
    "    model.eval()\n",
    "    ner_predictor_instance = GlobalPointerNERPredictor(model, ark_tokenizer, Ent2id)\n",
    "\n",
    "    predict_results = []\n",
    "    # try.txt改成你需要伪标的数据\n",
    "    with open('./datasets/preliminary_contest_datasets/preliminary_test_a/try.txt', 'r',\n",
    "              encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for _line in lines:\n",
    "            label = len(_line) * ['O']\n",
    "            for _preditc in ner_predictor_instance.predict_one_sample(_line[:-1]):\n",
    "                if 'I' in label[_preditc['start_idx']]:\n",
    "                    continue\n",
    "                if 'B' in label[_preditc['start_idx']] and 'O' not in label[_preditc['end_idx']]:\n",
    "                    continue\n",
    "                if 'O' in label[_preditc['start_idx']] and 'B' in label[_preditc['end_idx']]:\n",
    "                    continue\n",
    "\n",
    "                label[_preditc['start_idx']] = 'B-' + _preditc['type']\n",
    "                label[_preditc['start_idx'] + 1: _preditc['end_idx'] + 1] = (_preditc['end_idx'] - _preditc[\n",
    "                    'start_idx']) * [('I-' + _preditc['type'])]\n",
    "\n",
    "            predict_results.append([_line, label])\n",
    "\n",
    "\n",
    "    # new_df = pd.DataFrame({'line':[],'label':[]})\n",
    "    # for line in predict_results:\n",
    "    #     new_df.loc[len(new_df)] = line\n",
    "    # new_df.to_excel('./伪标数据/labels_{}fold.xlsx'.format(i), index=False)\n",
    "    with open('./伪标数据/labels_{}fold.txt'.format(i), 'w', encoding='UTF-8') as f:\n",
    "        f.write('line')\n",
    "        f.write('\\t')\n",
    "        f.write('label')\n",
    "        f.write('\\n')\n",
    "        for line in predict_results:\n",
    "            l = line[0].rstrip('\\n')\n",
    "            f.write(l)\n",
    "            r = str(line[1])\n",
    "            f.write('\\t')\n",
    "            f.write(r)\n",
    "            f.write('\\n')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "KF=5\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for i in range(1, KF+1):\n",
    "    temp_df = pd.read_csv('./伪标数据/labels_{}fold.txt'.format(i),sep='\\t')\n",
    "    df_list.append(temp_df)\n",
    "df1, df2, df3, df4, df5 = df_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "10000"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "with open('./伪标数据/best_label.txt', 'w', encoding='UTF-8') as f:\n",
    "    f.write('line')\n",
    "    f.write('\\t')\n",
    "    f.write('label')\n",
    "    f.write('\\n')\n",
    "\n",
    "    for i in range(len(df1)):\n",
    "        label1 = df1['label'][i]\n",
    "        label2 = df2['label'][i]\n",
    "        label3 = df3['label'][i]\n",
    "        label4 = df4['label'][i]\n",
    "        label5 = df5['label'][i]\n",
    "        labels = [label1, label2, label3, label4, label5]\n",
    "        # key: label的类型，起始位置索引\n",
    "        # value: 该label出现的次数\n",
    "        label_dict = {}\n",
    "        for x in labels:\n",
    "            if x not in label_dict.keys():\n",
    "                    label_dict[x] = 1\n",
    "            else:\n",
    "                label_dict[x] += 1\n",
    "        # 少数服从多数投票\n",
    "        true_entity = []\n",
    "        max_v = 0\n",
    "        for k,v in label_dict.items():\n",
    "            if v>=max_v:\n",
    "                max_v = v\n",
    "                true_entity = k\n",
    "\n",
    "        f.write(df1.loc[i]['line'])\n",
    "        f.write('\\t')\n",
    "        f.write(true_entity)\n",
    "        f.write('\\n')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('./伪标数据/best_label.txt',sep='\\t')\n",
    "new_df['label'] = new_df['label'].apply(lambda x: eval(x))\n",
    "new_array = np.array(new_df)\n",
    "new_list = new_array.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "with open('./伪标数据/pseudo_data.txt', 'w', encoding='utf-8') as f:\n",
    "    for _result in new_list:\n",
    "        for word, tag in zip(_result[0], _result[1]):\n",
    "            if word == '\\n':\n",
    "                continue\n",
    "            f.writelines(f'{word} {tag}\\n')\n",
    "        f.writelines('\\n')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}