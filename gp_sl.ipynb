{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from ark_nlp.model.ner.global_pointer_bert import Tokenizer\n",
    "from ark_nlp.model.ner.global_pointer_bert import Dataset as Dt\n",
    "import os\n",
    "import jieba\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import logging\n",
    "from tqdm import tqdm\n",
    "logging.set_verbosity_warning()\n",
    "\n",
    "from nezha import NeZhaConfig, NeZhaModel, NeZhaForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./my_nezha_cn_base2/ were not used when initializing NeZhaModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing NeZhaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NeZhaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of NeZhaModel were not initialized from the model checkpoint at ./my_nezha_cn_base2/ and are newly initialized: ['bert.encoder.layer.5.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.1.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.9.attention.self.relative_positions_encoding.positions_encoding', 'bert.pooler.dense.bias', 'bert.encoder.layer.6.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.11.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.7.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.3.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.0.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.4.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.8.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.10.attention.self.relative_positions_encoding.positions_encoding', 'bert.pooler.dense.weight', 'bert.encoder.layer.2.attention.self.relative_positions_encoding.positions_encoding']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "dropout_num = 0\n",
    "start = time.time()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "MAX_LEN = 128\n",
    "batch_size = 8\n",
    "EPOCHS = 6\n",
    "LR = 4e-5\n",
    "early_stop_epochs = 2\n",
    "model_path = './my_nezha_cn_base2/'\n",
    "model_save_path = 'model'\n",
    "\n",
    "# jieba.add_word('[CLS]')\n",
    "# jieba.add_word('[SEP]')\n",
    "# jieba.add_word('[unused1]')\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "# 设置随机数种子\n",
    "setup_seed(1998)\n",
    "\n",
    "config = NeZhaConfig.from_json_file(model_path + 'config.json')\n",
    "config.num_labels = 53\n",
    "# encoder = NeZhaForSequenceClassification(config)\n",
    "encoder = NeZhaModel.from_pretrained(model_path, config=config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "from ark_nlp.factory.utils.conlleval import get_entity_bio\n",
    "datalist = []\n",
    "with open('./datasets/preliminary_contest_datasets/train_data/train.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    lines.append('\\n')\n",
    "\n",
    "    text = []\n",
    "    labels = []\n",
    "    label_set = set()\n",
    "\n",
    "    for line in lines:\n",
    "        if line == '\\n':\n",
    "            text = ''.join(text)\n",
    "            entity_labels = []\n",
    "            for _type, _start_idx, _end_idx in get_entity_bio(labels, id2label=None):\n",
    "                entity_labels.append({\n",
    "                    'start_idx': _start_idx,\n",
    "                    'end_idx': _end_idx,\n",
    "                    'type': _type,\n",
    "                    'entity': text[_start_idx: _end_idx + 1]\n",
    "                })\n",
    "\n",
    "            if text == '':\n",
    "                continue\n",
    "\n",
    "            datalist.append({\n",
    "                'text': text,\n",
    "                'label': entity_labels,\n",
    "                'BIO': labels\n",
    "            })\n",
    "\n",
    "            text = []\n",
    "            labels = []\n",
    "\n",
    "        elif line == '  O\\n':\n",
    "            text.append(' ')\n",
    "            labels.append('O')\n",
    "        else:\n",
    "            line = line.strip('\\n').split()\n",
    "            if len(line) == 1:\n",
    "                term = ' '\n",
    "                label = line[0]\n",
    "            else:\n",
    "                term, label = line\n",
    "            text.append(term)\n",
    "            label_set.add(label.split('-')[-1])\n",
    "            labels.append(label)\n",
    "\n",
    "# 这里随意分割了一下看指标，建议实际使用sklearn分割或者交叉验证\n",
    "\n",
    "# train_data_df = pd.DataFrame(datalist)\n",
    "# train_data_df['label'] = train_data_df['label'].apply(lambda x: str(x))\n",
    "#\n",
    "# dev_data_df = pd.DataFrame(datalist[-400:])\n",
    "# dev_data_df['label'] = dev_data_df['label'].apply(lambda x: str(x))\n",
    "\n",
    "all_data = pd.DataFrame(datalist)\n",
    "all_data = all_data[:100]\n",
    "\n",
    "\n",
    "train_data_df, dev_data_df = train_test_split(all_data, test_size=0.1, random_state=1998)\n",
    "train_data_df['label'] = train_data_df['label'].apply(lambda x: str(x))\n",
    "dev_data_df['label'] = dev_data_df['label'].apply(lambda x: str(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "label_list = sorted(list(label_set))\n",
    "\n",
    "train_dataset = Dt(train_data_df, categories=label_list)\n",
    "dev_dataset = Dt(dev_data_df, categories=label_list)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "ark_tokenizer = Tokenizer(vocab=tokenizer, max_seq_len=128)\n",
    "\n",
    "train_dataset.convert_to_ids(ark_tokenizer)\n",
    "dev_dataset.convert_to_ids(ark_tokenizer)\n",
    "\n",
    "train_labels_id = []\n",
    "for i in range(len(train_dataset)):\n",
    "    train_labels_id.append(train_dataset[i]['label_ids'])\n",
    "dev_labels_id = []\n",
    "for i in range(len(dev_dataset)):\n",
    "    dev_labels_id.append(dev_dataset[i]['label_ids'])\n",
    "Ent2id = train_dataset.cat2id  # 53\n",
    "id2Ent = train_dataset.id2cat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\MACHEN~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.573 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载训练集标注实体词典\n",
      "词典大小为： 89624\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "print('加载训练集标注实体词典')\n",
    "jieba.load_userdict(\"./make_feature/训练集实体.txt\")  #加载词典，补充默认词典\n",
    "word2id = pd.read_pickle('./make_feature/word2id_4w.pkl')\n",
    "vocab_size = len(word2id)\n",
    "print('词典大小为：', vocab_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# tokenizer.ids_to_tokens[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len, ark_data):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.ark_data = ark_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        token_ids, at_mask, word_ids, start_ids, end_ids = self.get_token_ids(row)\n",
    "        return torch.tensor(token_ids, dtype=torch.long), torch.tensor(at_mask, dtype=torch.long), \\\n",
    "               torch.tensor(self.ark_data[index]['label_ids'].to_dense()), torch.tensor(self.ark_data[index]['token_type_ids'],dtype=torch.long),\\\n",
    "               torch.tensor(word_ids), torch.tensor(start_ids), torch.tensor(end_ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_token_ids(self, row):\n",
    "        sentence = row.text\n",
    "        tokens = self.tokenizer.tokenize(sentence)\n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "        padding = [0] * (self.max_len - len(tokens))\n",
    "        at_mask = [1] * len(tokens)\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        token_ids = token_ids + padding\n",
    "        at_mask = at_mask + padding\n",
    "\n",
    "\n",
    "        # *******************************引入词汇信息**********************************\n",
    "        sentence = sentence[:MAX_LEN]\n",
    "        word_ids = []\n",
    "        start_ids = []\n",
    "        end_ids = []\n",
    "        word_info_list = jieba.tokenize(sentence)\n",
    "        for word,start,end in word_info_list:\n",
    "            if word in word2id.keys():\n",
    "                word_ids.append(word2id[word])\n",
    "            else:\n",
    "                word_ids.append(word2id['<UNK>'])\n",
    "\n",
    "            # 因为有CLS在最前面，所以词的位置整体加1右移，且0位置用给padding，所以+2\n",
    "            start_ids.append(start+2)\n",
    "            end_ids.append(end-1+2)\n",
    "\n",
    "        padding = [word2id['<PAD>']] * (self.max_len - len(word_ids))\n",
    "        word_ids = word_ids + padding\n",
    "        # padding\n",
    "        start_ids = start_ids + [0] * (self.max_len - len(start_ids))\n",
    "        end_ids = end_ids + [0]  * (self.max_len - len(end_ids))\n",
    "\n",
    "        # **************************************************************************\n",
    "\n",
    "        return token_ids, at_mask, word_ids, start_ids, end_ids\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        token_ids = torch.stack([x[0] for x in batch])\n",
    "        at_mask = torch.stack([x[1] for x in batch])\n",
    "        labels = torch.stack([x[2] for x in batch])\n",
    "        token_type_ids = torch.stack([x[3] for x in batch])\n",
    "\n",
    "        word_ids = torch.stack([x[4] for x in batch])\n",
    "        start_ids = torch.stack([x[5] for x in batch])\n",
    "        end_ids = torch.stack([x[6] for x in batch])\n",
    "\n",
    "        return token_ids, at_mask, labels.squeeze(), token_type_ids, word_ids, start_ids, end_ids\n",
    "\n",
    "\n",
    "ner_train_dataset = Dataset(train_data_df, ark_tokenizer, MAX_LEN, train_dataset)\n",
    "ner_dev_dataset = Dataset(dev_data_df, ark_tokenizer, MAX_LEN, dev_dataset)\n",
    "\n",
    "train_loader = DataLoader(ner_train_dataset,  # 1250\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0,\n",
    "                          collate_fn=ner_train_dataset.collate_fn)\n",
    "dev_loader = DataLoader(ner_dev_dataset,  # 13\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        num_workers=0,\n",
    "                        collate_fn=ner_dev_dataset.collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# i=1\n",
    "# for sample in tqdm(train_loader):\n",
    "    # i+=1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# sample[6]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class FGM(object):\n",
    "    def __init__(self, module):\n",
    "        self.module = module\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(\n",
    "            self,\n",
    "            epsilon=1.,\n",
    "            emb_name='word_embeddings'\n",
    "    ):\n",
    "        for name, param in self.module.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0 and not torch.isnan(norm):\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(\n",
    "            self,\n",
    "            emb_name='word_embeddings'\n",
    "    ):\n",
    "        for name, param in self.module.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "\n",
    "\n",
    "class PGD():\n",
    "    def __init__(self, model, emb_name='word_embeddings', epsilon=1., alpha=0.3):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        self.model = model\n",
    "        self.emb_name = emb_name\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.emb_backup = {}\n",
    "        self.grad_backup = {}\n",
    "\n",
    "    def attack(self, is_first_attack=False):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and self.emb_name in name:\n",
    "                if is_first_attack:\n",
    "                    self.emb_backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0:\n",
    "                    r_at = self.alpha * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "                    param.data = self.project(name, param.data, self.epsilon)\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and self.emb_name in name:\n",
    "                assert name in self.emb_backup\n",
    "                param.data = self.emb_backup[name]\n",
    "        self.emb_backup = {}\n",
    "\n",
    "    def project(self, param_name, param_data, epsilon):\n",
    "        r = param_data - self.emb_backup[param_name]\n",
    "        if torch.norm(r) > epsilon:\n",
    "            r = epsilon * r / torch.norm(r)\n",
    "        return self.emb_backup[param_name] + r\n",
    "\n",
    "    def backup_grad(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                self.grad_backup[name] = param.grad.clone()\n",
    "\n",
    "    def restore_grad(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                param.grad = self.grad_backup[name]\n",
    "\n",
    "\n",
    "class EMA():\n",
    "    def __init__(self, model, decay):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "\n",
    "    def register(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def apply_shadow(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                self.backup[name] = param.data\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "\n",
    "\n",
    "class GlobalPointerCrossEntropy(nn.Module):\n",
    "    '''Multi-class Focal loss implementation'''\n",
    "\n",
    "    def __init__(self, ):\n",
    "        super(GlobalPointerCrossEntropy, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def multilabel_categorical_crossentropy(y_true, y_pred):\n",
    "        y_pred = (1 - 2 * y_true) * y_pred\n",
    "        y_pred_neg = y_pred - y_true * 1e12\n",
    "        y_pred_pos = y_pred - (1 - y_true) * 1e12\n",
    "        zeros = torch.zeros_like(y_pred[..., :1])\n",
    "        y_pred_neg = torch.cat([y_pred_neg, zeros], dim=-1)\n",
    "        y_pred_pos = torch.cat([y_pred_pos, zeros], dim=-1)\n",
    "        neg_loss = torch.logsumexp(y_pred_neg, dim=-1)\n",
    "        pos_loss = torch.logsumexp(y_pred_pos, dim=-1)\n",
    "\n",
    "        return neg_loss + pos_loss\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        logits: [N, C, L, L]\n",
    "        \"\"\"\n",
    "        bh = logits.shape[0] * logits.shape[1]\n",
    "        target = torch.reshape(target, (bh, -1))\n",
    "        logits = torch.reshape(logits, (bh, -1))\n",
    "        return torch.mean(GlobalPointerCrossEntropy.multilabel_categorical_crossentropy(target, logits))\n",
    "\n",
    "\n",
    "class MetricsCalculator(object):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_sample_f1(self, y_pred, y_true):\n",
    "        y_pred = torch.gt(y_pred, 0).float()\n",
    "        return 2 * torch.sum(y_true * y_pred) / torch.sum(y_true + y_pred)\n",
    "\n",
    "    def get_sample_precision(self, y_pred, y_true):\n",
    "        y_pred = torch.gt(y_pred, 0).float()\n",
    "        return torch.sum(y_pred[y_true == 1]) / (y_pred.sum() + 1)\n",
    "\n",
    "    def get_evaluate_fpr(self, y_pred, y_true):\n",
    "        y_pred = y_pred.cpu().numpy()\n",
    "        y_true = y_true.cpu().numpy()\n",
    "        pred = []\n",
    "        true = []\n",
    "        for b, l, start, end in zip(*np.where(y_pred > 0)):\n",
    "            pred.append((b, l, start, end))\n",
    "        for b, l, start, end in zip(*np.where(y_true > 0)):\n",
    "            true.append((b, l, start, end))\n",
    "\n",
    "        R = set(pred)\n",
    "        T = set(true)\n",
    "        X = len(R & T)\n",
    "        Y = len(R)\n",
    "        Z = len(T)\n",
    "        if Y != 0:\n",
    "            f1, precision, recall = 2 * X / (Y + Z), X / Y, X / Z\n",
    "        else:\n",
    "            f1, precision, recall = 2 * X / (Y + Z), 0, X / Z\n",
    "        return f1, precision, recall\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim_q, dim_k, dim_v):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.dim_q = dim_q\n",
    "        self.dim_k = dim_k\n",
    "        self.dim_v = dim_v\n",
    "\n",
    "        #定义线性变换函数\n",
    "        self.linear_q = nn.Linear(dim_q, dim_k, bias=False)\n",
    "        self.linear_k = nn.Linear(dim_q, dim_k, bias=False)\n",
    "        self.linear_v = nn.Linear(dim_q, dim_v, bias=False)\n",
    "        self._norm_fact = 1 / np.sqrt(dim_k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: batch, max_len, dim_q\n",
    "        #根据文本获得相应的维度\n",
    "\n",
    "        batch, n, dim_q = x.shape\n",
    "        assert dim_q == self.dim_q\n",
    "\n",
    "        q = self.linear_q(x)  # batch, max_len, dim_k\n",
    "        k = self.linear_k(x)  # batch, max_len, dim_k\n",
    "        v = self.linear_v(x)  # batch, max_len, dim_v\n",
    "        #q*k的转置 并*开根号后的dk\n",
    "        dist = torch.bmm(q, k.transpose(1, 2)) * self._norm_fact  # batch, max_len, max_len\n",
    "        #归一化获得attention的相关系数\n",
    "        dist = torch.softmax(dist, dim=-1)  # batch, max_len, max_len\n",
    "        #attention系数和v相乘，获得最终的得分\n",
    "        att = torch.bmm(dist, v)\n",
    "        return att\n",
    "\n",
    "class GlobalPointer(nn.Module):\n",
    "    def __init__(self, encoder, ent_type_size, inner_dim, RoPE=True):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.ent_type_size = ent_type_size\n",
    "        self.inner_dim = inner_dim\n",
    "        self.hidden_size = encoder.config.hidden_size\n",
    "        self.dense = nn.Linear(self.hidden_size, self.ent_type_size * self.inner_dim * 2)\n",
    "        # self.gru = nn.GRU(input_size=768,\n",
    "        #                   hidden_size=384,\n",
    "        #                   num_layers=1,\n",
    "        #                   batch_first=True,\n",
    "        #                   bidirectional=True)\n",
    "        #\n",
    "        # self.n_gram_fc = nn.Linear(768, 256)\n",
    "        # self.n_gram_tanh = nn.Tanh()\n",
    "\n",
    "        self.start_pos_embedding = nn.Embedding(MAX_LEN, 768, padding_idx=-1)\n",
    "        self.end_pos_embedding = nn.Embedding(MAX_LEN, 768, padding_idx=-1)\n",
    "        self.word_embedding = nn.Embedding(vocab_size, 768, padding_idx=word2id['<PAD>'])\n",
    "        self.LayerNorm = nn.LayerNorm(768)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.att = SelfAttention(768,768,768)\n",
    "\n",
    "        self.RoPE = RoPE\n",
    "\n",
    "    def sinusoidal_position_embedding(self, batch_size, seq_len, output_dim):\n",
    "        position_ids = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(-1)\n",
    "\n",
    "        indices = torch.arange(0, output_dim // 2, dtype=torch.float)\n",
    "        indices = torch.pow(10000, -2 * indices / output_dim)\n",
    "        embeddings = position_ids * indices\n",
    "        embeddings = torch.stack([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)\n",
    "        embeddings = embeddings.repeat((batch_size, *([1] * len(embeddings.shape))))\n",
    "        embeddings = torch.reshape(embeddings, (batch_size, seq_len, output_dim))\n",
    "        embeddings = embeddings.to(self.device)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, word_ids, start_ids, end_ids):\n",
    "\n",
    "\n",
    "        context_outputs = self.encoder(input_ids, attention_mask, token_type_ids)\n",
    "        last_hidden_state = context_outputs[0]\n",
    "        # last_hidden_state.shape = (bs, seq_len, 768)\n",
    "        batch_size = last_hidden_state.size()[0]\n",
    "        seq_len = last_hidden_state.size()[1]\n",
    "\n",
    "        start_position_ids = torch.arange(start=1,end=MAX_LEN+1, dtype=torch.long, device=input_ids.device)\n",
    "        start_position_ids = start_position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        start_position_ids = torch.cat((start_position_ids, start_ids), dim=-1)\n",
    "        start_position_embeddings = self.start_pos_embedding(start_position_ids)\n",
    "\n",
    "\n",
    "\n",
    "        end_position_ids = torch.arange(start=1,end=MAX_LEN+1, dtype=torch.long, device=input_ids.device)\n",
    "        end_position_ids = end_position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        end_position_ids = torch.cat((end_position_ids, end_ids), dim=-1)\n",
    "        end_position_embeddings = self.end_pos_embedding(end_position_ids)\n",
    "\n",
    "        self.device = input_ids.device\n",
    "\n",
    "\n",
    "        # ***********************************词汇融合******************************************************\n",
    "        word_feats = self.word_embedding(word_ids)\n",
    "        last_hidden_state  = torch.cat((last_hidden_state, word_feats), dim=1)\n",
    "        last_hidden_state = last_hidden_state + start_position_embeddings + end_position_embeddings\n",
    "        last_hidden_state = self.LayerNorm(last_hidden_state)\n",
    "        last_hidden_state = self.dropout(last_hidden_state)\n",
    "        last_hidden_state = self.att(last_hidden_state)\n",
    "        # ************************************************************************************************\n",
    "        last_hidden_state = last_hidden_state[:,:128,:]\n",
    "\n",
    "        print(last_hidden_state.shape)\n",
    "        print('yo')\n",
    "\n",
    "\n",
    "\n",
    "        # outputs:(batch_size, seq_len, ent_type_size*inner_dim*2)\n",
    "        outputs = self.dense(last_hidden_state)\n",
    "        outputs = torch.split(outputs, self.inner_dim * 2, dim=-1)\n",
    "        # outputs:(batch_size, seq_len, ent_type_size, inner_dim*2)\n",
    "        outputs = torch.stack(outputs, dim=-2)\n",
    "        # qw,kw:(batch_size, seq_len, ent_type_size, inner_dim)\n",
    "        qw, kw = outputs[..., :self.inner_dim], outputs[..., self.inner_dim:]  # TODO:修改为Linear获取？\n",
    "\n",
    "        if self.RoPE:\n",
    "            # pos_emb:(batch_size, seq_len, inner_dim)\n",
    "            pos_emb = self.sinusoidal_position_embedding(batch_size, seq_len, self.inner_dim)\n",
    "            # cos_pos,sin_pos: (batch_size, seq_len, 1, inner_dim)\n",
    "            cos_pos = pos_emb[..., None, 1::2].repeat_interleave(2, dim=-1)\n",
    "            sin_pos = pos_emb[..., None, ::2].repeat_interleave(2, dim=-1)\n",
    "            qw2 = torch.stack([-qw[..., 1::2], qw[..., ::2]], -1)\n",
    "            qw2 = qw2.reshape(qw.shape)\n",
    "            qw = qw * cos_pos + qw2 * sin_pos\n",
    "            kw2 = torch.stack([-kw[..., 1::2], kw[..., ::2]], -1)\n",
    "            kw2 = kw2.reshape(kw.shape)\n",
    "            kw = kw * cos_pos + kw2 * sin_pos\n",
    "\n",
    "        # logits:(batch_size, ent_type_size, seq_len, seq_len)\n",
    "        logits = torch.einsum('bmhd,bnhd->bhmn', qw, kw)\n",
    "\n",
    "        # padding mask\n",
    "        pad_mask = attention_mask.unsqueeze(1).unsqueeze(1).expand(batch_size, self.ent_type_size, seq_len, seq_len)\n",
    "        # pad_mask_h = attention_mask.unsqueeze(1).unsqueeze(-1).expand(batch_size, self.ent_type_size, seq_len, seq_len)\n",
    "        # pad_mask = pad_mask_v&pad_mask_h\n",
    "        logits = logits * pad_mask - (1 - pad_mask) * 1e12\n",
    "\n",
    "        # 排除下三角\n",
    "        mask = torch.tril(torch.ones_like(logits), -1)\n",
    "        logits = logits - mask * 1e12\n",
    "\n",
    "        return logits / self.inner_dim ** 0.5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "model = GlobalPointer(encoder, len(Ent2id), 64)  # (encoder, ent_type_size, inner_dim)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "def global_pointer_f1_score(y_true, y_pred):\n",
    "    y_pred = torch.gt(y_pred, 0)\n",
    "    return torch.sum(y_true * y_pred).item(), torch.sum(y_true + y_pred).item()\n",
    "\n",
    "\n",
    "def validation_fn(model, dev_loader, loss_fn):\n",
    "    model.eval()\n",
    "    ema.apply_shadow()\n",
    "    total_loss = []\n",
    "    cnt = 0\n",
    "    total_f1_, total_precision_, total_recall_ = 0., 0., 0.\n",
    "    with torch.no_grad():\n",
    "        for token_id, at_mask, label_id, token_type_ids, word_ids, start_ids, end_ids in tqdm(dev_loader):\n",
    "            outputs = model(token_id.to(device), at_mask.to(device), token_type_ids.to(device),\n",
    "                            word_ids.to(device), start_ids.to(device), end_ids.to(device))\n",
    "            loss = loss_fn(outputs, label_id.to(device))\n",
    "            total_loss.append(loss.item())\n",
    "            cnt += 1\n",
    "            f1, p, r = metrics.get_evaluate_fpr(outputs, label_id.to(device))\n",
    "            total_f1_ += f1\n",
    "            total_precision_ += p\n",
    "            total_recall_ += r\n",
    "        avg_f1 = total_f1_ / cnt\n",
    "        avg_precision = total_precision_ / cnt\n",
    "        avg_recall = total_recall_ / cnt\n",
    "        # t_loss = np.array(total_loss).mean()\n",
    "    # ema.restore()\n",
    "    return avg_f1, avg_precision, avg_recall\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, dev_loader, model_save_path='../outputs',\n",
    "                early_stop_epochs=2, is_fgm=True, is_pgd=False):\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    ########优化器 学习率\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "         {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "     ]\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=LR, eps=1e-8)\n",
    "\n",
    "    loss_fn = GlobalPointerCrossEntropy().to(device)\n",
    "\n",
    "\n",
    "    best_vmetric = 0\n",
    "    if is_fgm:\n",
    "        fgm = FGM(module=model)\n",
    "\n",
    "    if is_pgd:\n",
    "        pgd = PGD(model=model)\n",
    "        K = 3\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = []\n",
    "        total_f1 = []\n",
    "        model.train()\n",
    "        #bar = tqdm_notebook(train_loader)\n",
    "\n",
    "        idx = -1\n",
    "        for (token_id, at_mask, label_id, token_type_ids, word_ids, start_ids, end_ids) in tqdm(train_loader):\n",
    "            idx+=1\n",
    "            outputs = model(token_id.to(device), at_mask.to(device), token_type_ids.to(device),\n",
    "                            word_ids.to(device), start_ids.to(device), end_ids.to(device))\n",
    "            loss = loss_fn(outputs, label_id.to(device))\n",
    "            total_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            #bar.set_postfix(loss=loss.item())\n",
    "            if is_fgm:\n",
    "                fgm.attack()\n",
    "                outputs = model(token_id.to(device), at_mask.to(device), token_type_ids.to(device),\n",
    "                                word_ids.to(device), start_ids.to(device), end_ids.to(device))\n",
    "                loss_fgm = loss_fn(outputs, label_id.to(device))\n",
    "                loss_fgm.backward()\n",
    "                fgm.restore()\n",
    "            if is_pgd:\n",
    "                pgd.backup_grad()\n",
    "                for t in range(K):\n",
    "                    pgd.attack(is_first_attack=(t == 0))\n",
    "                    if t != K - 1:\n",
    "                        model.zero_grad()\n",
    "                    else:\n",
    "                        pgd.restore_grad()\n",
    "                    outputs = model(token_id.to(device), at_mask.to(device), token_type_ids.to(device),\n",
    "                                    word_ids.to(device), start_ids.to(device), end_ids.to(device))\n",
    "                    loss_pgd = loss_fn(outputs, label_id.to(device))\n",
    "                    loss_pgd.backward()\n",
    "                pgd.restore()\n",
    "            f1 = metrics.get_sample_f1(outputs, label_id.to(device))\n",
    "            total_f1.append(f1.item())\n",
    "            if ((idx + 1) % 4) == 0:\n",
    "                # optimizer the net\n",
    "                optimizer.step()  # update parameters of net\n",
    "                # ema.update()\n",
    "                optimizer.zero_grad()  # reset gradient\n",
    "                ema.update()\n",
    "        ema.apply_shadow()\n",
    "        t_loss = np.array(total_loss).mean()\n",
    "        t_f1 = np.array(total_f1).mean()\n",
    "        avg_f1, avg_precision, avg_recall = validation_fn(model, dev_loader, loss_fn)\n",
    "        print('epoch:{},训练集损失t_loss:{:.6f},准确率pre:{:.6f},召回率:{:.6f},F1_eval:{:.6f}'.format(epoch, t_loss,\n",
    "                                                                                           avg_precision, avg_recall,\n",
    "                                                                                           avg_f1))\n",
    "\n",
    "        model_save_path = './model/bm.bin'\n",
    "        if avg_f1 > best_vmetric:\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            best_vmetric = avg_f1\n",
    "            no_improve = 0\n",
    "            print('improve save model!!!')\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "\n",
    "        if no_improve_epochs == early_stop_epochs:\n",
    "            print('no improve score !!! stop train !!!')\n",
    "            break\n",
    "        ema.restore()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1/12 [00:09<01:41,  9.26s/it]\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 128, 768])\n",
      "yo\n",
      "torch.Size([8, 128, 768])\n",
      "yo\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\ML_ENVS\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3441, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\MACHEN~1\\AppData\\Local\\Temp/ipykernel_26056/3415419276.py\", line 4, in <module>\n",
      "    train_model(model, train_loader, dev_loader, early_stop_epochs=early_stop_epochs)\n",
      "  File \"C:\\Users\\MACHEN~1\\AppData\\Local\\Temp/ipykernel_26056/1312349741.py\", line 70, in train_model\n",
      "    word_ids.to(device), start_ids.to(device), end_ids.to(device))\n",
      "  File \"F:\\ML_ENVS\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1051, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\MACHEN~1\\AppData\\Local\\Temp/ipykernel_26056/2536937711.py\", line 245, in forward\n",
      "    context_outputs = self.encoder(input_ids, attention_mask, token_type_ids)\n",
      "  File \"F:\\ML_ENVS\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1051, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"E:\\打工\\竞赛\\GAIIC\\nezha.py\", line 677, in forward\n",
      "    encoder_attention_mask=encoder_extended_attention_mask,\n",
      "  File \"F:\\ML_ENVS\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1051, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"E:\\打工\\竞赛\\GAIIC\\nezha.py\", line 506, in forward\n",
      "    hidden_states, attention_mask, head_mask[i], encoder_hidden_states, encoder_attention_mask\n",
      "  File \"F:\\ML_ENVS\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1051, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"E:\\打工\\竞赛\\GAIIC\\nezha.py\", line 479, in forward\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "  File \"F:\\ML_ENVS\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1051, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"F:\\ML_ENVS\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\", line 426, in forward\n",
      "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
      "  File \"F:\\ML_ENVS\\lib\\site-packages\\torch\\nn\\functional.py\", line 1555, in gelu\n",
      "    return torch._C._nn.gelu(input)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\ML_ENVS\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\ML_ENVS\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"F:\\ML_ENVS\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"F:\\ML_ENVS\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"F:\\ML_ENVS\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"F:\\ML_ENVS\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"F:\\ML_ENVS\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"F:\\ML_ENVS\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"F:\\ML_ENVS\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"F:\\ML_ENVS\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"F:\\ML_ENVS\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "    \u001B[1;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "\u001B[1;32mC:\\Users\\MACHEN~1\\AppData\\Local\\Temp/ipykernel_26056/3415419276.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mmetrics\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mMetricsCalculator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0mtrain_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdev_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mearly_stop_epochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mearly_stop_epochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Users\\MACHEN~1\\AppData\\Local\\Temp/ipykernel_26056/1312349741.py\u001B[0m in \u001B[0;36mtrain_model\u001B[1;34m(model, train_loader, dev_loader, model_save_path, early_stop_epochs, is_fgm, is_pgd)\u001B[0m\n\u001B[0;32m     69\u001B[0m             outputs = model(token_id.to(device), at_mask.to(device), token_type_ids.to(device),\n\u001B[1;32m---> 70\u001B[1;33m                             word_ids.to(device), start_ids.to(device), end_ids.to(device))\n\u001B[0m\u001B[0;32m     71\u001B[0m             \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mloss_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabel_id\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\ML_ENVS\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Users\\MACHEN~1\\AppData\\Local\\Temp/ipykernel_26056/2536937711.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, word_ids, start_ids, end_ids)\u001B[0m\n\u001B[0;32m    244\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 245\u001B[1;33m         \u001B[0mcontext_outputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mencoder\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattention_mask\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtoken_type_ids\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    246\u001B[0m         \u001B[0mlast_hidden_state\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcontext_outputs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\ML_ENVS\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\打工\\竞赛\\GAIIC\\nezha.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001B[0m\n\u001B[0;32m    676\u001B[0m             \u001B[0mencoder_hidden_states\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mencoder_hidden_states\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 677\u001B[1;33m             \u001B[0mencoder_attention_mask\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mencoder_extended_attention_mask\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    678\u001B[0m         )\n",
      "\u001B[1;32mF:\\ML_ENVS\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\打工\\竞赛\\GAIIC\\nezha.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001B[0m\n\u001B[0;32m    505\u001B[0m             layer_outputs = layer_module(\n\u001B[1;32m--> 506\u001B[1;33m                 \u001B[0mhidden_states\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattention_mask\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhead_mask\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoder_hidden_states\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoder_attention_mask\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    507\u001B[0m             )\n",
      "\u001B[1;32mF:\\ML_ENVS\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\打工\\竞赛\\GAIIC\\nezha.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001B[0m\n\u001B[0;32m    478\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 479\u001B[1;33m         \u001B[0mintermediate_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mintermediate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mattention_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    480\u001B[0m         \u001B[0mlayer_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moutput\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mintermediate_output\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattention_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\ML_ENVS\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\ML_ENVS\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states)\u001B[0m\n\u001B[0;32m    425\u001B[0m         \u001B[0mhidden_states\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdense\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 426\u001B[1;33m         \u001B[0mhidden_states\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mintermediate_act_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    427\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mhidden_states\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\ML_ENVS\\lib\\site-packages\\torch\\nn\\functional.py\u001B[0m in \u001B[0;36mgelu\u001B[1;34m(input)\u001B[0m\n\u001B[0;32m   1554\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mhandle_torch_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgelu\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1555\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_C\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_nn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgelu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1556\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32mF:\\ML_ENVS\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001B[0m in \u001B[0;36mshowtraceback\u001B[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[0;32m   2060\u001B[0m                         \u001B[1;31m# in the engines. This should return a list of strings.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2061\u001B[1;33m                         \u001B[0mstb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_render_traceback_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2062\u001B[0m                     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "    \u001B[1;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "\u001B[1;32mF:\\ML_ENVS\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001B[0m in \u001B[0;36mshowtraceback\u001B[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[0;32m   2062\u001B[0m                     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2063\u001B[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001B[1;32m-> 2064\u001B[1;33m                                             value, tb, tb_offset=tb_offset)\n\u001B[0m\u001B[0;32m   2065\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2066\u001B[0m                     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_showtraceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0metype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\ML_ENVS\\lib\\site-packages\\IPython\\core\\ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[0;32m   1366\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1367\u001B[0m         return FormattedTB.structured_traceback(\n\u001B[1;32m-> 1368\u001B[1;33m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001B[0m\u001B[0;32m   1369\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1370\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\ML_ENVS\\lib\\site-packages\\IPython\\core\\ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[0;32m   1266\u001B[0m             \u001B[1;31m# Verbose modes need a full traceback\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1267\u001B[0m             return VerboseTB.structured_traceback(\n\u001B[1;32m-> 1268\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0metype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtb\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtb_offset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnumber_of_lines_of_context\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1269\u001B[0m             )\n\u001B[0;32m   1270\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0mmode\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m'Minimal'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\ML_ENVS\\lib\\site-packages\\IPython\\core\\ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[0;32m   1123\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1124\u001B[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001B[1;32m-> 1125\u001B[1;33m                                                                tb_offset)\n\u001B[0m\u001B[0;32m   1126\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1127\u001B[0m         \u001B[0mcolors\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mColors\u001B[0m  \u001B[1;31m# just a shorthand + quicker name lookup\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\ML_ENVS\\lib\\site-packages\\IPython\\core\\ultratb.py\u001B[0m in \u001B[0;36mformat_exception_as_a_whole\u001B[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001B[0m\n\u001B[0;32m   1080\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1081\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1082\u001B[1;33m         \u001B[0mlast_unique\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrecursion_repeat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfind_recursion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0morig_etype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrecords\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1083\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1084\u001B[0m         \u001B[0mframes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat_records\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrecords\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlast_unique\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrecursion_repeat\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\ML_ENVS\\lib\\site-packages\\IPython\\core\\ultratb.py\u001B[0m in \u001B[0;36mfind_recursion\u001B[1;34m(etype, value, records)\u001B[0m\n\u001B[0;32m    380\u001B[0m     \u001B[1;31m# first frame (from in to out) that looks different.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    381\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mis_recursion_error\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0metype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrecords\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 382\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrecords\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    383\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    384\u001B[0m     \u001B[1;31m# Select filename, lineno, func_name to track frames with\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "ema = EMA(model, 0.995)\n",
    "ema.register()\n",
    "metrics = MetricsCalculator()\n",
    "train_model(model, train_loader, dev_loader, early_stop_epochs=early_stop_epochs)\n",
    "\n",
    "end = time.time()\n",
    "print('共用时:', (end - start) / 60, '分钟')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}